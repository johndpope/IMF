# Model parameters
model:
  latent_dim: 32
  base_channels: 64
  num_layers: 4

# Training parameters
training:
  batch_size: 4
  num_epochs: 100
  learning_rate_g: 2e-4
  learning_rate_d: 2e-4
  ema_decay: 0.999
  style_mixing_prob: 0.9
  noise_magnitude: 0.01
  lambda_pixel: 10
  lambda_perceptual: 10
  lambda_adv: 1
  n_critic: 1  # Number of discriminator updates per generator update

# Dataset parameters
dataset:
  root_dir: "/media/oem/12TB/Downloads/CelebV-HQ/celebvhq/35666"
  frame_skip: 1

# Checkpointing
checkpoints:
  dir: "./checkpoints"
  interval: 10

# Logging and visualization
logging:
  sample_interval: 1
  sample_size: 8
  output_dir: "./samples"

# Accelerator settings
accelerator:
  mixed_precision: "fp16"  # Options: "no", "fp16", "bf16"
  cpu: false
  num_processes: 1  # Set to more than 1 for multi-GPU training

# Discriminator parameters
discriminator:
  ndf: 64  # Number of filters in the first conv layer

# Optimizer parameters
optimizer:
  beta1: 0.5
  beta2: 0.999

# Loss function
loss:
  type: "wasserstein"  # Changed to Wasserstein loss for WGAN-GP%