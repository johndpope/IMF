# Model parameters
model:
  latent_dim: 32
  base_channels: 64
  num_layers: 4

# Training parameters
training:
  every_xref_frames: 16
  batch_size: 1
  num_epochs: 100
  save_steps: 25
  learning_rate_g: 1.0e-4 # Reduced learning rate for generator
  learning_rate_d: 1.0e-5  # Reduced learning rate for discriminator
  # learning_rate_g: 5.0e-4  # Increased learning rate for generator
  # learning_rate_d: 5.0e-4  # Increased learning rate for discriminator
  ema_decay: 0.999
  style_mixing_prob: 0.9
  noise_magnitude: 0.01
  gradient_accumulation_steps: 4
  lambda_pixel: 10  # in paper lambda-pixel = 10 Adjust this value as needed
  lambda_perceptual: 10 # lambda perceptual = 10
  lambda_adv: 1 # adverserial = 1
  lambda_gp: 10  # Gradient penalty coefficient
  lambda_mse: 1.0
  n_critic: 2  # Number of discriminator updates per generator update
  clip_grad_norm: 1.0  # Maximum norm for gradient clipping
  r1_gamma: 10
  r1_interval: 16
  label_smoothing: 0.1

  # dynamic discriminator learning to overcome mode collapse
  initial_learning_rate_d: 1.0e-4
  min_learning_rate_d: 1.0e-6
  max_learning_rate_d: 1.0e-3
  d_lr_adjust_frequency: 100  # Adjust D learning rate every 100 steps
  d_lr_adjust_factor: 2.0  # Factor to increase/decrease D learning rate
  target_d_loss_ratio: 0.6  # Target ratio of D loss to G loss


# Dataset parameters
dataset:
  # celeb-hq torrent https://github.com/johndpope/MegaPortrait-hack/tree/main/junk
  root_dir: "/media/oem/12TB/Downloads/CelebV-HQ/celebvhq/35666" # for overfitting M2Ohb0FAaJU_1.mp4 use https://github.com/johndpope/MegaPortrait-hack/tree/main/junk
  json_file: './data/celebvhq_info.json' 
  # json_file: './data/celebvhq_info.json' # 35k
  max_frames: 50

# Checkpointing
checkpoints:
  dir: "./checkpoints"
  interval: 10

# Logging and visualization
logging:
  sample_interval: 1
  sample_size: 8
  output_dir: "./samples"

# Accelerator settings
accelerator:
  mixed_precision: "fp16"  # Options: "no", "fp16", "bf16"
  cpu: false
  num_processes: 1  # Set to more than 1 for multi-GPU training

# Discriminator parameters
discriminator:
  ndf: 64  # Number of filters in the first conv layer

# Optimizer parameters
optimizer:
  beta1: 0.5
  beta2: 0.999

# Loss function
loss:
  type: "wasserstein"  # Changed to Wasserstein loss for WGAN-GP