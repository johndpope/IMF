
Testing gradient flow for IMFModel
âš¾ DenseFeatureEncoder input shape: torch.Size([1, 3, 256, 256])
    After initial conv: torch.Size([1, 64, 256, 256])
    After down_block 1: torch.Size([1, 64, 128, 128])
    After down_block 2: torch.Size([1, 128, 64, 64])
    After down_block 3: torch.Size([1, 256, 32, 32])
    After down_block 4: torch.Size([1, 512, 16, 16])
    After down_block 5: torch.Size([1, 512, 8, 8])
    DenseFeatureEncoder output shapes: [torch.Size([1, 128, 64, 64]), torch.Size([1, 256, 32, 32]), torch.Size([1, 512, 16, 16]), torch.Size([1, 512, 8, 8])]
LatentTokenEncoder input shape: torch.Size([1, 3, 256, 256])
After initial conv and activation: torch.Size([1, 64, 256, 256])
After res_block 1: torch.Size([1, 128, 128, 128])
After res_block 2: torch.Size([1, 256, 64, 64])
After res_block 3: torch.Size([1, 512, 32, 32])
After res_block 4: torch.Size([1, 512, 16, 16])
After res_block 5: torch.Size([1, 512, 8, 8])
After res_block 6: torch.Size([1, 512, 4, 4])
After equalconv: torch.Size([1, 512, 4, 4])
After global average pooling: torch.Size([1, 512])
After linear layer 1: torch.Size([1, 512])
After linear layer 2: torch.Size([1, 512])
After linear layer 3: torch.Size([1, 512])
After linear layer 4: torch.Size([1, 512])
Final output: torch.Size([1, 32])
LatentTokenEncoder input shape: torch.Size([1, 3, 256, 256])
After initial conv and activation: torch.Size([1, 64, 256, 256])
After res_block 1: torch.Size([1, 128, 128, 128])
After res_block 2: torch.Size([1, 256, 64, 64])
After res_block 3: torch.Size([1, 512, 32, 32])
After res_block 4: torch.Size([1, 512, 16, 16])
After res_block 5: torch.Size([1, 512, 8, 8])
After res_block 6: torch.Size([1, 512, 4, 4])
After equalconv: torch.Size([1, 512, 4, 4])
After global average pooling: torch.Size([1, 512])
After linear layer 1: torch.Size([1, 512])
After linear layer 2: torch.Size([1, 512])
After linear layer 3: torch.Size([1, 512])
After linear layer 4: torch.Size([1, 512])
Final output: torch.Size([1, 32])
ðŸŽ’ FrameDecoder input shapes
f:torch.Size([1, 128, 64, 64])
f:torch.Size([1, 256, 32, 32])
f:torch.Size([1, 512, 16, 16])
f:torch.Size([1, 512, 8, 8])
Reshaped features: [torch.Size([1, 128, 64, 64]), torch.Size([1, 256, 32, 32]), torch.Size([1, 512, 16, 16]), torch.Size([1, 512, 8, 8])]
    Initial x shape: torch.Size([1, 512, 8, 8])

    Processing upconv_block 1
    After upconv_block 1: torch.Size([1, 512, 16, 16])
    Processing feat_block 1
    feat_block 1 input shape: torch.Size([1, 512, 16, 16])
    feat_block 1 output shape: torch.Size([1, 512, 16, 16])
    Concatenating: x torch.Size([1, 512, 16, 16]) and feat torch.Size([1, 512, 16, 16])
    After concatenation: torch.Size([1, 1024, 16, 16])

    Processing upconv_block 2
    After upconv_block 2: torch.Size([1, 512, 32, 32])
    Processing feat_block 2
    feat_block 2 input shape: torch.Size([1, 256, 32, 32])
    feat_block 2 output shape: torch.Size([1, 256, 32, 32])
    Concatenating: x torch.Size([1, 512, 32, 32]) and feat torch.Size([1, 256, 32, 32])
    After concatenation: torch.Size([1, 768, 32, 32])

    Processing upconv_block 3
    After upconv_block 3: torch.Size([1, 256, 64, 64])
    Processing feat_block 3
    feat_block 3 input shape: torch.Size([1, 128, 64, 64])
    feat_block 3 output shape: torch.Size([1, 128, 64, 64])
    Concatenating: x torch.Size([1, 256, 64, 64]) and feat torch.Size([1, 128, 64, 64])
    After concatenation: torch.Size([1, 384, 64, 64])

    Processing upconv_block 4
    After upconv_block 4: torch.Size([1, 128, 128, 128])

    Processing upconv_block 5
    After upconv_block 5: torch.Size([1, 64, 256, 256])

    Applying final convolution
    FrameDecoder final output shape: torch.Size([1, 3, 256, 256])
latent_token_encoder.conv1.weight: gradient norm = 11.320090
latent_token_encoder.conv1.bias: gradient norm = 1.302591
latent_token_encoder.res_blocks.0.conv1.conv.weight: gradient norm = 35.192299
latent_token_encoder.res_blocks.0.conv1.conv.bias: gradient norm = 0.000006
latent_token_encoder.res_blocks.0.conv1.bn.weight: gradient norm = 0.598824
latent_token_encoder.res_blocks.0.conv1.bn.bias: gradient norm = 0.571394
latent_token_encoder.res_blocks.0.conv2.conv.weight: gradient norm = 40.653366
latent_token_encoder.res_blocks.0.conv2.conv.bias: gradient norm = 0.000003
latent_token_encoder.res_blocks.0.conv2.bn.weight: gradient norm = 0.596908
latent_token_encoder.res_blocks.0.conv2.bn.bias: gradient norm = 0.471818
latent_token_encoder.res_blocks.0.skip_conv.conv.weight: gradient norm = 29.697824
latent_token_encoder.res_blocks.0.skip_conv.conv.bias: gradient norm = 0.000006
latent_token_encoder.res_blocks.0.skip_conv.bn.weight: gradient norm = 0.647219
latent_token_encoder.res_blocks.0.skip_conv.bn.bias: gradient norm = 0.563982
latent_token_encoder.res_blocks.1.conv1.conv.weight: gradient norm = 37.058834
latent_token_encoder.res_blocks.1.conv1.conv.bias: gradient norm = 0.000001
latent_token_encoder.res_blocks.1.conv1.bn.weight: gradient norm = 0.529294
latent_token_encoder.res_blocks.1.conv1.bn.bias: gradient norm = 0.432100
latent_token_encoder.res_blocks.1.conv2.conv.weight: gradient norm = 43.283562
latent_token_encoder.res_blocks.1.conv2.conv.bias: gradient norm = 0.000001
latent_token_encoder.res_blocks.1.conv2.bn.weight: gradient norm = 0.460657
latent_token_encoder.res_blocks.1.conv2.bn.bias: gradient norm = 0.343939
latent_token_encoder.res_blocks.1.skip_conv.conv.weight: gradient norm = 30.393908
latent_token_encoder.res_blocks.1.skip_conv.conv.bias: gradient norm = 0.000001
latent_token_encoder.res_blocks.1.skip_conv.bn.weight: gradient norm = 0.408579
latent_token_encoder.res_blocks.1.skip_conv.bn.bias: gradient norm = 0.340287
latent_token_encoder.res_blocks.2.conv1.conv.weight: gradient norm = 39.043488
latent_token_encoder.res_blocks.2.conv1.conv.bias: gradient norm = 0.000000
latent_token_encoder.res_blocks.2.conv1.bn.weight: gradient norm = 0.392250
latent_token_encoder.res_blocks.2.conv1.bn.bias: gradient norm = 0.339671
latent_token_encoder.res_blocks.2.conv2.conv.weight: gradient norm = 45.689297
latent_token_encoder.res_blocks.2.conv2.conv.bias: gradient norm = 0.000000
latent_token_encoder.res_blocks.2.conv2.bn.weight: gradient norm = 0.328779
latent_token_encoder.res_blocks.2.conv2.bn.bias: gradient norm = 0.265127
latent_token_encoder.res_blocks.2.skip_conv.conv.weight: gradient norm = 32.098759
latent_token_encoder.res_blocks.2.skip_conv.conv.bias: gradient norm = 0.000000
latent_token_encoder.res_blocks.2.skip_conv.bn.weight: gradient norm = 0.305273
latent_token_encoder.res_blocks.2.skip_conv.bn.bias: gradient norm = 0.261218
latent_token_encoder.res_blocks.3.conv1.conv.weight: gradient norm = 41.664135
latent_token_encoder.res_blocks.3.conv1.conv.bias: gradient norm = 0.000000
latent_token_encoder.res_blocks.3.conv1.bn.weight: gradient norm = 0.282246
latent_token_encoder.res_blocks.3.conv1.bn.bias: gradient norm = 0.257811
latent_token_encoder.res_blocks.3.conv2.conv.weight: gradient norm = 34.451809
latent_token_encoder.res_blocks.3.conv2.conv.bias: gradient norm = 0.000000
latent_token_encoder.res_blocks.3.conv2.bn.weight: gradient norm = 0.250673
latent_token_encoder.res_blocks.3.conv2.bn.bias: gradient norm = 0.212399
latent_token_encoder.res_blocks.3.skip_conv.conv.weight: gradient norm = 34.148746
latent_token_encoder.res_blocks.3.skip_conv.conv.bias: gradient norm = 0.000000
latent_token_encoder.res_blocks.3.skip_conv.bn.weight: gradient norm = 0.249347
latent_token_encoder.res_blocks.3.skip_conv.bn.bias: gradient norm = 0.207034
latent_token_encoder.res_blocks.4.conv1.conv.weight: gradient norm = 31.698423
latent_token_encoder.res_blocks.4.conv1.conv.bias: gradient norm = 0.000000
latent_token_encoder.res_blocks.4.conv1.bn.weight: gradient norm = 0.229714
latent_token_encoder.res_blocks.4.conv1.bn.bias: gradient norm = 0.203783
latent_token_encoder.res_blocks.4.conv2.conv.weight: gradient norm = 26.827261
latent_token_encoder.res_blocks.4.conv2.conv.bias: gradient norm = 0.000000
latent_token_encoder.res_blocks.4.conv2.bn.weight: gradient norm = 0.198378
latent_token_encoder.res_blocks.4.conv2.bn.bias: gradient norm = 0.168535
latent_token_encoder.res_blocks.4.skip_conv.conv.weight: gradient norm = 26.452568
latent_token_encoder.res_blocks.4.skip_conv.conv.bias: gradient norm = 0.000000
latent_token_encoder.res_blocks.4.skip_conv.bn.weight: gradient norm = 0.204645
latent_token_encoder.res_blocks.4.skip_conv.bn.bias: gradient norm = 0.180799
latent_token_encoder.res_blocks.5.conv1.conv.weight: gradient norm = 25.184984
latent_token_encoder.res_blocks.5.conv1.conv.bias: gradient norm = 0.000000
latent_token_encoder.res_blocks.5.conv1.bn.weight: gradient norm = 0.194987
latent_token_encoder.res_blocks.5.conv1.bn.bias: gradient norm = 0.181515
latent_token_encoder.res_blocks.5.conv2.conv.weight: gradient norm = 23.415251
latent_token_encoder.res_blocks.5.conv2.conv.bias: gradient norm = 0.000000
latent_token_encoder.res_blocks.5.conv2.bn.weight: gradient norm = 0.679544
latent_token_encoder.res_blocks.5.conv2.bn.bias: gradient norm = 0.828077
latent_token_encoder.res_blocks.5.skip_conv.conv.weight: gradient norm = 23.278402
latent_token_encoder.res_blocks.5.skip_conv.conv.bias: gradient norm = 0.000000
latent_token_encoder.res_blocks.5.skip_conv.bn.weight: gradient norm = 0.669718
latent_token_encoder.res_blocks.5.skip_conv.bn.bias: gradient norm = 0.814979
latent_token_encoder.equalconv.conv.bias: gradient norm = 1.513508
latent_token_encoder.equalconv.conv.weight_orig: gradient norm = 1.269233
latent_token_encoder.linear_layers.0.linear.bias: gradient norm = 1.082299
latent_token_encoder.linear_layers.0.linear.weight_orig: gradient norm = 1.281194
latent_token_encoder.linear_layers.1.linear.bias: gradient norm = 1.083161
latent_token_encoder.linear_layers.1.linear.weight_orig: gradient norm = 1.365153
latent_token_encoder.linear_layers.2.linear.bias: gradient norm = 1.175855
latent_token_encoder.linear_layers.2.linear.weight_orig: gradient norm = 1.566979
latent_token_encoder.linear_layers.3.linear.bias: gradient norm = 1.122202
latent_token_encoder.linear_layers.3.linear.weight_orig: gradient norm = 1.604452
latent_token_encoder.final_linear.linear.bias: gradient norm = 1.085345
latent_token_encoder.final_linear.linear.weight_orig: gradient norm = 1.654073
latent_token_decoder.const: gradient norm = 97.677322
latent_token_decoder.style_conv_layers.0.conv.weight: gradient norm = 103.677361
latent_token_decoder.style_conv_layers.0.style.weight: gradient norm = 637.967407
latent_token_decoder.style_conv_layers.0.style.bias: gradient norm = 816.861206
latent_token_decoder.style_conv_layers.1.conv.weight: gradient norm = 116.547768
latent_token_decoder.style_conv_layers.1.style.weight: gradient norm = 712.670349
latent_token_decoder.style_conv_layers.1.style.bias: gradient norm = 903.644470
latent_token_decoder.style_conv_layers.2.conv.weight: gradient norm = 112.437096
latent_token_decoder.style_conv_layers.2.style.weight: gradient norm = 706.033447
latent_token_decoder.style_conv_layers.2.style.bias: gradient norm = 912.103760
latent_token_decoder.style_conv_layers.3.conv.weight: gradient norm = 108.083435
latent_token_decoder.style_conv_layers.3.style.weight: gradient norm = 646.467590
latent_token_decoder.style_conv_layers.3.style.bias: gradient norm = 814.138550
latent_token_decoder.style_conv_layers.4.conv.weight: gradient norm = 114.735664
latent_token_decoder.style_conv_layers.4.style.weight: gradient norm = 702.971863
latent_token_decoder.style_conv_layers.4.style.bias: gradient norm = 884.571960
latent_token_decoder.style_conv_layers.5.conv.weight: gradient norm = 112.581055
latent_token_decoder.style_conv_layers.5.style.weight: gradient norm = 683.582458
latent_token_decoder.style_conv_layers.5.style.bias: gradient norm = 865.454529
latent_token_decoder.style_conv_layers.6.conv.weight: gradient norm = 106.337189
latent_token_decoder.style_conv_layers.6.style.weight: gradient norm = 667.077759
latent_token_decoder.style_conv_layers.6.style.bias: gradient norm = 826.534668
latent_token_decoder.style_conv_layers.7.conv.weight: gradient norm = 13.221072
latent_token_decoder.style_conv_layers.7.style.weight: gradient norm = 70.980080
latent_token_decoder.style_conv_layers.7.style.bias: gradient norm = 88.377274
latent_token_decoder.style_conv_layers.8.conv.weight: gradient norm = 13.097474
latent_token_decoder.style_conv_layers.8.style.weight: gradient norm = 74.613533
latent_token_decoder.style_conv_layers.8.style.bias: gradient norm = 90.828224
latent_token_decoder.style_conv_layers.9.conv.weight: gradient norm = 12.330044
latent_token_decoder.style_conv_layers.9.style.weight: gradient norm = 77.935249
latent_token_decoder.style_conv_layers.9.style.bias: gradient norm = 94.628799
latent_token_decoder.style_conv_layers.10.conv.weight: gradient norm = 1.301443
latent_token_decoder.style_conv_layers.10.style.weight: gradient norm = 8.505334
latent_token_decoder.style_conv_layers.10.style.bias: gradient norm = 11.019262
latent_token_decoder.style_conv_layers.11.conv.weight: gradient norm = 1.418244
latent_token_decoder.style_conv_layers.11.style.weight: gradient norm = 6.965727
latent_token_decoder.style_conv_layers.11.style.bias: gradient norm = 8.955790
latent_token_decoder.style_conv_layers.12.conv.weight: gradient norm = 1.536332
latent_token_decoder.style_conv_layers.12.style.weight: gradient norm = 8.992867
latent_token_decoder.style_conv_layers.12.style.bias: gradient norm = 11.907469
dense_feature_encoder.initial_conv.0.weight: gradient norm = 1261954.125000
dense_feature_encoder.initial_conv.0.bias: gradient norm = 0.528200
dense_feature_encoder.initial_conv.1.weight: gradient norm = 42890.601562
dense_feature_encoder.initial_conv.1.bias: gradient norm = 40748.906250
dense_feature_encoder.down_blocks.0.conv1.weight: gradient norm = 2045675.625000
dense_feature_encoder.down_blocks.0.conv1.bias: gradient norm = 0.577734
dense_feature_encoder.down_blocks.0.bn1.weight: gradient norm = 43814.976562
dense_feature_encoder.down_blocks.0.bn1.bias: gradient norm = 40440.164062
dense_feature_encoder.down_blocks.0.conv2.weight: gradient norm = 1723571.750000
dense_feature_encoder.down_blocks.0.conv2.bias: gradient norm = 37497.964844
dense_feature_encoder.down_blocks.0.feat_res_block1.bn1.weight: gradient norm = 34612.054688
dense_feature_encoder.down_blocks.0.feat_res_block1.bn1.bias: gradient norm = 28645.166016
dense_feature_encoder.down_blocks.0.feat_res_block1.conv1.weight: gradient norm = 1316390.125000
dense_feature_encoder.down_blocks.0.feat_res_block1.conv1.bias: gradient norm = 0.181676
dense_feature_encoder.down_blocks.0.feat_res_block1.bn2.weight: gradient norm = 30390.326172
dense_feature_encoder.down_blocks.0.feat_res_block1.bn2.bias: gradient norm = 27264.349609
dense_feature_encoder.down_blocks.0.feat_res_block1.conv2.weight: gradient norm = 1147479.625000
dense_feature_encoder.down_blocks.0.feat_res_block1.conv2.bias: gradient norm = 37497.945312
dense_feature_encoder.down_blocks.0.feat_res_block2.bn1.weight: gradient norm = 16371.284180
dense_feature_encoder.down_blocks.0.feat_res_block2.bn1.bias: gradient norm = 9709.594727
dense_feature_encoder.down_blocks.0.feat_res_block2.conv1.weight: gradient norm = 751728.500000
dense_feature_encoder.down_blocks.0.feat_res_block2.conv1.bias: gradient norm = 0.087378
dense_feature_encoder.down_blocks.0.feat_res_block2.bn2.weight: gradient norm = 17468.033203
dense_feature_encoder.down_blocks.0.feat_res_block2.bn2.bias: gradient norm = 14630.524414
dense_feature_encoder.down_blocks.0.feat_res_block2.conv2.weight: gradient norm = 659186.937500
dense_feature_encoder.down_blocks.0.feat_res_block2.conv2.bias: gradient norm = 21049.035156
dense_feature_encoder.down_blocks.1.conv1.weight: gradient norm = 726457.062500
dense_feature_encoder.down_blocks.1.conv1.bias: gradient norm = 0.203926
dense_feature_encoder.down_blocks.1.bn1.weight: gradient norm = 15529.459961
dense_feature_encoder.down_blocks.1.bn1.bias: gradient norm = 13290.531250
dense_feature_encoder.down_blocks.1.conv2.weight: gradient norm = 884565.125000
dense_feature_encoder.down_blocks.1.conv2.bias: gradient norm = 15043.881836
dense_feature_encoder.down_blocks.1.feat_res_block1.bn1.weight: gradient norm = 9918.655273
dense_feature_encoder.down_blocks.1.feat_res_block1.bn1.bias: gradient norm = 8931.614258
dense_feature_encoder.down_blocks.1.feat_res_block1.conv1.weight: gradient norm = 674186.562500
dense_feature_encoder.down_blocks.1.feat_res_block1.conv1.bias: gradient norm = 0.026552
dense_feature_encoder.down_blocks.1.feat_res_block1.bn2.weight: gradient norm = 10514.752930
dense_feature_encoder.down_blocks.1.feat_res_block1.bn2.bias: gradient norm = 9401.004883
dense_feature_encoder.down_blocks.1.feat_res_block1.conv2.weight: gradient norm = 592075.250000
dense_feature_encoder.down_blocks.1.feat_res_block1.conv2.bias: gradient norm = 15043.884766
dense_feature_encoder.down_blocks.1.feat_res_block2.bn1.weight: gradient norm = 6842.926270
dense_feature_encoder.down_blocks.1.feat_res_block2.bn1.bias: gradient norm = 4114.761230
dense_feature_encoder.down_blocks.1.feat_res_block2.conv1.weight: gradient norm = 390124.812500
dense_feature_encoder.down_blocks.1.feat_res_block2.conv1.bias: gradient norm = 0.013663
dense_feature_encoder.down_blocks.1.feat_res_block2.bn2.weight: gradient norm = 5605.596191
dense_feature_encoder.down_blocks.1.feat_res_block2.bn2.bias: gradient norm = 5269.239746
dense_feature_encoder.down_blocks.1.feat_res_block2.conv2.weight: gradient norm = 344331.781250
dense_feature_encoder.down_blocks.1.feat_res_block2.conv2.bias: gradient norm = 8606.774414
dense_feature_encoder.down_blocks.2.conv1.weight: gradient norm = 392616.437500
dense_feature_encoder.down_blocks.2.conv1.bias: gradient norm = 0.043205
dense_feature_encoder.down_blocks.2.bn1.weight: gradient norm = 5644.379395
dense_feature_encoder.down_blocks.2.bn1.bias: gradient norm = 5424.253418
dense_feature_encoder.down_blocks.2.conv2.weight: gradient norm = 482836.343750
dense_feature_encoder.down_blocks.2.conv2.bias: gradient norm = 5319.142090
dense_feature_encoder.down_blocks.2.feat_res_block1.bn1.weight: gradient norm = 4363.792969
dense_feature_encoder.down_blocks.2.feat_res_block1.bn1.bias: gradient norm = 3858.229736
dense_feature_encoder.down_blocks.2.feat_res_block1.conv1.weight: gradient norm = 369035.843750
dense_feature_encoder.down_blocks.2.feat_res_block1.conv1.bias: gradient norm = 0.005482
dense_feature_encoder.down_blocks.2.feat_res_block1.bn2.weight: gradient norm = 3692.497070
dense_feature_encoder.down_blocks.2.feat_res_block1.bn2.bias: gradient norm = 3563.835693
dense_feature_encoder.down_blocks.2.feat_res_block1.conv2.weight: gradient norm = 321398.812500
dense_feature_encoder.down_blocks.2.feat_res_block1.conv2.bias: gradient norm = 5319.141602
dense_feature_encoder.down_blocks.2.feat_res_block2.bn1.weight: gradient norm = 2592.855713
dense_feature_encoder.down_blocks.2.feat_res_block2.bn1.bias: gradient norm = 1626.711792
dense_feature_encoder.down_blocks.2.feat_res_block2.conv1.weight: gradient norm = 218663.015625
dense_feature_encoder.down_blocks.2.feat_res_block2.conv1.bias: gradient norm = 0.002826
dense_feature_encoder.down_blocks.2.feat_res_block2.bn2.weight: gradient norm = 2311.634277
dense_feature_encoder.down_blocks.2.feat_res_block2.bn2.bias: gradient norm = 2285.262939
dense_feature_encoder.down_blocks.2.feat_res_block2.conv2.weight: gradient norm = 194630.562500
dense_feature_encoder.down_blocks.2.feat_res_block2.conv2.bias: gradient norm = 3578.514160
dense_feature_encoder.down_blocks.3.conv1.weight: gradient norm = 216363.968750
dense_feature_encoder.down_blocks.3.conv1.bias: gradient norm = 0.007331
dense_feature_encoder.down_blocks.3.bn1.weight: gradient norm = 2331.440186
dense_feature_encoder.down_blocks.3.bn1.bias: gradient norm = 2060.851807
dense_feature_encoder.down_blocks.3.conv2.weight: gradient norm = 278709.406250
dense_feature_encoder.down_blocks.3.conv2.bias: gradient norm = 2808.988525
dense_feature_encoder.down_blocks.3.feat_res_block1.bn1.weight: gradient norm = 1753.826050
dense_feature_encoder.down_blocks.3.feat_res_block1.bn1.bias: gradient norm = 1576.393188
dense_feature_encoder.down_blocks.3.feat_res_block1.conv1.weight: gradient norm = 208968.453125
dense_feature_encoder.down_blocks.3.feat_res_block1.conv1.bias: gradient norm = 0.001112
dense_feature_encoder.down_blocks.3.feat_res_block1.bn2.weight: gradient norm = 1643.449463
dense_feature_encoder.down_blocks.3.feat_res_block1.bn2.bias: gradient norm = 1535.514404
dense_feature_encoder.down_blocks.3.feat_res_block1.conv2.weight: gradient norm = 188005.406250
dense_feature_encoder.down_blocks.3.feat_res_block1.conv2.bias: gradient norm = 2808.988525
dense_feature_encoder.down_blocks.3.feat_res_block2.bn1.weight: gradient norm = 1016.050171
dense_feature_encoder.down_blocks.3.feat_res_block2.bn1.bias: gradient norm = 594.455688
dense_feature_encoder.down_blocks.3.feat_res_block2.conv1.weight: gradient norm = 123049.718750
dense_feature_encoder.down_blocks.3.feat_res_block2.conv1.bias: gradient norm = 0.000549
dense_feature_encoder.down_blocks.3.feat_res_block2.bn2.weight: gradient norm = 1030.693726
dense_feature_encoder.down_blocks.3.feat_res_block2.bn2.bias: gradient norm = 1057.232300
dense_feature_encoder.down_blocks.3.feat_res_block2.conv2.weight: gradient norm = 124434.140625
dense_feature_encoder.down_blocks.3.feat_res_block2.conv2.bias: gradient norm = 2772.211426
dense_feature_encoder.down_blocks.4.conv1.weight: gradient norm = 17605.542969
dense_feature_encoder.down_blocks.4.conv1.bias: gradient norm = 0.000225
dense_feature_encoder.down_blocks.4.bn1.weight: gradient norm = 186.101883
dense_feature_encoder.down_blocks.4.bn1.bias: gradient norm = 206.837646
dense_feature_encoder.down_blocks.4.conv2.weight: gradient norm = 22415.679688
dense_feature_encoder.down_blocks.4.conv2.bias: gradient norm = 683.605408
dense_feature_encoder.down_blocks.4.feat_res_block1.bn1.weight: gradient norm = 102.955162
dense_feature_encoder.down_blocks.4.feat_res_block1.bn1.bias: gradient norm = 85.672539
dense_feature_encoder.down_blocks.4.feat_res_block1.conv1.weight: gradient norm = 12537.026367
dense_feature_encoder.down_blocks.4.feat_res_block1.conv1.bias: gradient norm = 0.000046
dense_feature_encoder.down_blocks.4.feat_res_block1.bn2.weight: gradient norm = 170.146027
dense_feature_encoder.down_blocks.4.feat_res_block1.bn2.bias: gradient norm = 184.894623
dense_feature_encoder.down_blocks.4.feat_res_block1.conv2.weight: gradient norm = 19280.949219
dense_feature_encoder.down_blocks.4.feat_res_block1.conv2.bias: gradient norm = 683.605408
dense_feature_encoder.down_blocks.4.feat_res_block2.bn1.weight: gradient norm = 64.609283
dense_feature_encoder.down_blocks.4.feat_res_block2.bn1.bias: gradient norm = 38.744553
dense_feature_encoder.down_blocks.4.feat_res_block2.conv1.weight: gradient norm = 7630.462402
dense_feature_encoder.down_blocks.4.feat_res_block2.conv1.bias: gradient norm = 0.000031
dense_feature_encoder.down_blocks.4.feat_res_block2.bn2.weight: gradient norm = 222.754822
dense_feature_encoder.down_blocks.4.feat_res_block2.bn2.bias: gradient norm = 271.142609
dense_feature_encoder.down_blocks.4.feat_res_block2.conv2.weight: gradient norm = 24712.933594
dense_feature_encoder.down_blocks.4.feat_res_block2.conv2.bias: gradient norm = 1020.370422
implicit_motion_alignment.0.cross_attention.to_q.weight: gradient norm = 7172.759277
implicit_motion_alignment.0.cross_attention.to_q.bias: gradient norm = 299.454865
implicit_motion_alignment.0.cross_attention.to_k.weight: gradient norm = 8212.823242
implicit_motion_alignment.0.cross_attention.to_k.bias: gradient norm = 0.000764
implicit_motion_alignment.0.cross_attention.to_v.weight: gradient norm = 5681.493652
implicit_motion_alignment.0.cross_attention.to_v.bias: gradient norm = 866.093262
implicit_motion_alignment.0.cross_attention.to_out.weight: gradient norm = 6450.944336
implicit_motion_alignment.0.cross_attention.to_out.bias: gradient norm = 1444.495117
implicit_motion_alignment.0.blocks.0.attention.in_proj_weight: gradient norm = 3082.971436
implicit_motion_alignment.0.blocks.0.attention.in_proj_bias: gradient norm = 272.653687
implicit_motion_alignment.0.blocks.0.attention.out_proj.weight: gradient norm = 3865.456055
implicit_motion_alignment.0.blocks.0.attention.out_proj.bias: gradient norm = 452.623993
implicit_motion_alignment.0.blocks.0.mlp.0.weight: gradient norm = 2978.205566
implicit_motion_alignment.0.blocks.0.mlp.0.bias: gradient norm = 174.592377
implicit_motion_alignment.0.blocks.0.mlp.2.weight: gradient norm = 5632.219238
implicit_motion_alignment.0.blocks.0.mlp.2.bias: gradient norm = 393.436218
implicit_motion_alignment.0.blocks.0.norm1.weight: gradient norm = 204.647385
implicit_motion_alignment.0.blocks.0.norm1.bias: gradient norm = 194.814240
implicit_motion_alignment.0.blocks.0.norm2.weight: gradient norm = 149.777344
implicit_motion_alignment.0.blocks.0.norm2.bias: gradient norm = 103.171776
implicit_motion_alignment.0.blocks.1.attention.in_proj_weight: gradient norm = 1947.355347
implicit_motion_alignment.0.blocks.1.attention.in_proj_bias: gradient norm = 172.135742
implicit_motion_alignment.0.blocks.1.attention.out_proj.weight: gradient norm = 2403.857666
implicit_motion_alignment.0.blocks.1.attention.out_proj.bias: gradient norm = 315.627258
implicit_motion_alignment.0.blocks.1.mlp.0.weight: gradient norm = 2625.773682
implicit_motion_alignment.0.blocks.1.mlp.0.bias: gradient norm = 147.157471
implicit_motion_alignment.0.blocks.1.mlp.2.weight: gradient norm = 4314.665527
implicit_motion_alignment.0.blocks.1.mlp.2.bias: gradient norm = 296.901855
implicit_motion_alignment.0.blocks.1.norm1.weight: gradient norm = 113.220306
implicit_motion_alignment.0.blocks.1.norm1.bias: gradient norm = 107.932404
implicit_motion_alignment.0.blocks.1.norm2.weight: gradient norm = 130.719666
implicit_motion_alignment.0.blocks.1.norm2.bias: gradient norm = 88.862740
implicit_motion_alignment.0.blocks.2.attention.in_proj_weight: gradient norm = 1651.054443
implicit_motion_alignment.0.blocks.2.attention.in_proj_bias: gradient norm = 145.939697
implicit_motion_alignment.0.blocks.2.attention.out_proj.weight: gradient norm = 1908.379150
implicit_motion_alignment.0.blocks.2.attention.out_proj.bias: gradient norm = 262.678467
implicit_motion_alignment.0.blocks.2.mlp.0.weight: gradient norm = 1903.335938
implicit_motion_alignment.0.blocks.2.mlp.0.bias: gradient norm = 111.045517
implicit_motion_alignment.0.blocks.2.mlp.2.weight: gradient norm = 3564.890137
implicit_motion_alignment.0.blocks.2.mlp.2.bias: gradient norm = 232.527222
implicit_motion_alignment.0.blocks.2.norm1.weight: gradient norm = 86.301308
implicit_motion_alignment.0.blocks.2.norm1.bias: gradient norm = 95.616722
implicit_motion_alignment.0.blocks.2.norm2.weight: gradient norm = 93.158737
implicit_motion_alignment.0.blocks.2.norm2.bias: gradient norm = 68.619881
implicit_motion_alignment.0.blocks.3.attention.in_proj_weight: gradient norm = 1267.642090
implicit_motion_alignment.0.blocks.3.attention.in_proj_bias: gradient norm = 112.047752
implicit_motion_alignment.0.blocks.3.attention.out_proj.weight: gradient norm = 1529.242432
implicit_motion_alignment.0.blocks.3.attention.out_proj.bias: gradient norm = 202.893173
implicit_motion_alignment.0.blocks.3.mlp.0.weight: gradient norm = 1553.427856
implicit_motion_alignment.0.blocks.3.mlp.0.bias: gradient norm = 86.845612
implicit_motion_alignment.0.blocks.3.mlp.2.weight: gradient norm = 2833.397217
implicit_motion_alignment.0.blocks.3.mlp.2.bias: gradient norm = 182.641647
implicit_motion_alignment.0.blocks.3.norm1.weight: gradient norm = 79.112938
implicit_motion_alignment.0.blocks.3.norm1.bias: gradient norm = 76.958519
implicit_motion_alignment.0.blocks.3.norm2.weight: gradient norm = 81.617523
implicit_motion_alignment.0.blocks.3.norm2.bias: gradient norm = 57.876919
implicit_motion_alignment.1.cross_attention.to_q.weight: gradient norm = 9160.849609
implicit_motion_alignment.1.cross_attention.to_q.bias: gradient norm = 147.528275
implicit_motion_alignment.1.cross_attention.to_k.weight: gradient norm = 8823.687500
implicit_motion_alignment.1.cross_attention.to_k.bias: gradient norm = 0.003274
implicit_motion_alignment.1.cross_attention.to_v.weight: gradient norm = 15215.882812
implicit_motion_alignment.1.cross_attention.to_v.bias: gradient norm = 2034.159180
implicit_motion_alignment.1.cross_attention.to_out.weight: gradient norm = 14636.551758
implicit_motion_alignment.1.cross_attention.to_out.bias: gradient norm = 3487.177490
implicit_motion_alignment.1.blocks.0.attention.in_proj_weight: gradient norm = 8962.036133
implicit_motion_alignment.1.blocks.0.attention.in_proj_bias: gradient norm = 560.366089
implicit_motion_alignment.1.blocks.0.attention.out_proj.weight: gradient norm = 10652.517578
implicit_motion_alignment.1.blocks.0.attention.out_proj.bias: gradient norm = 924.748474
implicit_motion_alignment.1.blocks.0.mlp.0.weight: gradient norm = 6645.636230
implicit_motion_alignment.1.blocks.0.mlp.0.bias: gradient norm = 330.510010
implicit_motion_alignment.1.blocks.0.mlp.2.weight: gradient norm = 12979.674805
implicit_motion_alignment.1.blocks.0.mlp.2.bias: gradient norm = 867.364197
implicit_motion_alignment.1.blocks.0.norm1.weight: gradient norm = 359.250061
implicit_motion_alignment.1.blocks.0.norm1.bias: gradient norm = 384.146454
implicit_motion_alignment.1.blocks.0.norm2.weight: gradient norm = 242.397125
implicit_motion_alignment.1.blocks.0.norm2.bias: gradient norm = 199.539566
implicit_motion_alignment.1.blocks.1.attention.in_proj_weight: gradient norm = 6216.146973
implicit_motion_alignment.1.blocks.1.attention.in_proj_bias: gradient norm = 388.518127
implicit_motion_alignment.1.blocks.1.attention.out_proj.weight: gradient norm = 7833.398926
implicit_motion_alignment.1.blocks.1.attention.out_proj.bias: gradient norm = 672.448364
implicit_motion_alignment.1.blocks.1.mlp.0.weight: gradient norm = 4997.746582
implicit_motion_alignment.1.blocks.1.mlp.0.bias: gradient norm = 247.142502
implicit_motion_alignment.1.blocks.1.mlp.2.weight: gradient norm = 9446.717773
implicit_motion_alignment.1.blocks.1.mlp.2.bias: gradient norm = 613.891663
implicit_motion_alignment.1.blocks.1.norm1.weight: gradient norm = 258.326782
implicit_motion_alignment.1.blocks.1.norm1.bias: gradient norm = 258.351593
implicit_motion_alignment.1.blocks.1.norm2.weight: gradient norm = 182.523193
implicit_motion_alignment.1.blocks.1.norm2.bias: gradient norm = 141.543350
implicit_motion_alignment.1.blocks.2.attention.in_proj_weight: gradient norm = 5159.766113
implicit_motion_alignment.1.blocks.2.attention.in_proj_bias: gradient norm = 322.489685
implicit_motion_alignment.1.blocks.2.attention.out_proj.weight: gradient norm = 5649.913086
implicit_motion_alignment.1.blocks.2.attention.out_proj.bias: gradient norm = 525.410339
implicit_motion_alignment.1.blocks.2.mlp.0.weight: gradient norm = 3930.154785
implicit_motion_alignment.1.blocks.2.mlp.0.bias: gradient norm = 195.256027
implicit_motion_alignment.1.blocks.2.mlp.2.weight: gradient norm = 7043.860840
implicit_motion_alignment.1.blocks.2.mlp.2.bias: gradient norm = 493.602417
implicit_motion_alignment.1.blocks.2.norm1.weight: gradient norm = 231.309479
implicit_motion_alignment.1.blocks.2.norm1.bias: gradient norm = 234.348450
implicit_motion_alignment.1.blocks.2.norm2.weight: gradient norm = 163.887009
implicit_motion_alignment.1.blocks.2.norm2.bias: gradient norm = 117.587914
implicit_motion_alignment.1.blocks.3.attention.in_proj_weight: gradient norm = 4141.009277
implicit_motion_alignment.1.blocks.3.attention.in_proj_bias: gradient norm = 258.815552
implicit_motion_alignment.1.blocks.3.attention.out_proj.weight: gradient norm = 5263.742676
implicit_motion_alignment.1.blocks.3.attention.out_proj.bias: gradient norm = 444.193542
implicit_motion_alignment.1.blocks.3.mlp.0.weight: gradient norm = 3443.606934
implicit_motion_alignment.1.blocks.3.mlp.0.bias: gradient norm = 171.457214
implicit_motion_alignment.1.blocks.3.mlp.2.weight: gradient norm = 6313.919434
implicit_motion_alignment.1.blocks.3.mlp.2.bias: gradient norm = 436.431549
implicit_motion_alignment.1.blocks.3.norm1.weight: gradient norm = 189.105286
implicit_motion_alignment.1.blocks.3.norm1.bias: gradient norm = 187.003067
implicit_motion_alignment.1.blocks.3.norm2.weight: gradient norm = 130.518982
implicit_motion_alignment.1.blocks.3.norm2.bias: gradient norm = 98.740410
implicit_motion_alignment.2.cross_attention.to_q.weight: gradient norm = 57918.445312
implicit_motion_alignment.2.cross_attention.to_q.bias: gradient norm = 616.383240
implicit_motion_alignment.2.cross_attention.to_k.weight: gradient norm = 56672.425781
implicit_motion_alignment.2.cross_attention.to_k.bias: gradient norm = 0.001941
implicit_motion_alignment.2.cross_attention.to_v.weight: gradient norm = 65670.023438
implicit_motion_alignment.2.cross_attention.to_v.bias: gradient norm = 6943.051270
implicit_motion_alignment.2.cross_attention.to_out.weight: gradient norm = 68886.468750
implicit_motion_alignment.2.cross_attention.to_out.bias: gradient norm = 12067.115234
implicit_motion_alignment.2.blocks.0.attention.in_proj_weight: gradient norm = 44729.753906
implicit_motion_alignment.2.blocks.0.attention.in_proj_bias: gradient norm = 1977.625977
implicit_motion_alignment.2.blocks.0.attention.out_proj.weight: gradient norm = 55193.621094
implicit_motion_alignment.2.blocks.0.attention.out_proj.bias: gradient norm = 3414.075439
implicit_motion_alignment.2.blocks.0.mlp.0.weight: gradient norm = 31777.855469
implicit_motion_alignment.2.blocks.0.mlp.0.bias: gradient norm = 1163.918091
implicit_motion_alignment.2.blocks.0.mlp.2.weight: gradient norm = 56904.546875
implicit_motion_alignment.2.blocks.0.mlp.2.bias: gradient norm = 3038.209229
implicit_motion_alignment.2.blocks.0.norm1.weight: gradient norm = 1271.711304
implicit_motion_alignment.2.blocks.0.norm1.bias: gradient norm = 1395.896851
implicit_motion_alignment.2.blocks.0.norm2.weight: gradient norm = 771.165527
implicit_motion_alignment.2.blocks.0.norm2.bias: gradient norm = 640.822021
implicit_motion_alignment.2.blocks.1.attention.in_proj_weight: gradient norm = 30322.958984
implicit_motion_alignment.2.blocks.1.attention.in_proj_bias: gradient norm = 1340.140869
implicit_motion_alignment.2.blocks.1.attention.out_proj.weight: gradient norm = 37086.679688
implicit_motion_alignment.2.blocks.1.attention.out_proj.bias: gradient norm = 2287.069336
implicit_motion_alignment.2.blocks.1.mlp.0.weight: gradient norm = 22606.705078
implicit_motion_alignment.2.blocks.1.mlp.0.bias: gradient norm = 843.317383
implicit_motion_alignment.2.blocks.1.mlp.2.weight: gradient norm = 41132.113281
implicit_motion_alignment.2.blocks.1.mlp.2.bias: gradient norm = 2140.683594
implicit_motion_alignment.2.blocks.1.norm1.weight: gradient norm = 971.328491
implicit_motion_alignment.2.blocks.1.norm1.bias: gradient norm = 954.319824
implicit_motion_alignment.2.blocks.1.norm2.weight: gradient norm = 622.454407
implicit_motion_alignment.2.blocks.1.norm2.bias: gradient norm = 487.610809
implicit_motion_alignment.2.blocks.2.attention.in_proj_weight: gradient norm = 23788.753906
implicit_motion_alignment.2.blocks.2.attention.in_proj_bias: gradient norm = 1051.346802
implicit_motion_alignment.2.blocks.2.attention.out_proj.weight: gradient norm = 28509.636719
implicit_motion_alignment.2.blocks.2.attention.out_proj.bias: gradient norm = 1809.996948
implicit_motion_alignment.2.blocks.2.mlp.0.weight: gradient norm = 18232.632812
implicit_motion_alignment.2.blocks.2.mlp.0.bias: gradient norm = 672.084900
implicit_motion_alignment.2.blocks.2.mlp.2.weight: gradient norm = 33210.898438
implicit_motion_alignment.2.blocks.2.mlp.2.bias: gradient norm = 1748.233521
implicit_motion_alignment.2.blocks.2.norm1.weight: gradient norm = 739.391113
implicit_motion_alignment.2.blocks.2.norm1.bias: gradient norm = 730.960815
implicit_motion_alignment.2.blocks.2.norm2.weight: gradient norm = 455.869904
implicit_motion_alignment.2.blocks.2.norm2.bias: gradient norm = 376.657715
implicit_motion_alignment.2.blocks.3.attention.in_proj_weight: gradient norm = 20037.000000
implicit_motion_alignment.2.blocks.3.attention.in_proj_bias: gradient norm = 885.533203
implicit_motion_alignment.2.blocks.3.attention.out_proj.weight: gradient norm = 23792.023438
implicit_motion_alignment.2.blocks.3.attention.out_proj.bias: gradient norm = 1512.649170
implicit_motion_alignment.2.blocks.3.mlp.0.weight: gradient norm = 15432.144531
implicit_motion_alignment.2.blocks.3.mlp.0.bias: gradient norm = 558.646545
implicit_motion_alignment.2.blocks.3.mlp.2.weight: gradient norm = 28600.699219
implicit_motion_alignment.2.blocks.3.mlp.2.bias: gradient norm = 1457.054077
implicit_motion_alignment.2.blocks.3.norm1.weight: gradient norm = 642.360168
implicit_motion_alignment.2.blocks.3.norm1.bias: gradient norm = 619.804382
implicit_motion_alignment.2.blocks.3.norm2.weight: gradient norm = 409.155731
implicit_motion_alignment.2.blocks.3.norm2.bias: gradient norm = 322.425232
implicit_motion_alignment.3.cross_attention.to_q.weight: gradient norm = 1614.265991
implicit_motion_alignment.3.cross_attention.to_q.bias: gradient norm = 37.667973
implicit_motion_alignment.3.cross_attention.to_k.weight: gradient norm = 1529.219116
implicit_motion_alignment.3.cross_attention.to_k.bias: gradient norm = 0.000025
implicit_motion_alignment.3.cross_attention.to_v.weight: gradient norm = 20481.875000
implicit_motion_alignment.3.cross_attention.to_v.bias: gradient norm = 2778.997559
implicit_motion_alignment.3.cross_attention.to_out.weight: gradient norm = 20950.460938
implicit_motion_alignment.3.cross_attention.to_out.bias: gradient norm = 4880.413086
implicit_motion_alignment.3.blocks.0.attention.in_proj_weight: gradient norm = 16689.347656
implicit_motion_alignment.3.blocks.0.attention.in_proj_bias: gradient norm = 739.324341
implicit_motion_alignment.3.blocks.0.attention.out_proj.weight: gradient norm = 20577.023438
implicit_motion_alignment.3.blocks.0.attention.out_proj.bias: gradient norm = 1304.239380
implicit_motion_alignment.3.blocks.0.mlp.0.weight: gradient norm = 9882.261719
implicit_motion_alignment.3.blocks.0.mlp.0.bias: gradient norm = 435.892151
implicit_motion_alignment.3.blocks.0.mlp.2.weight: gradient norm = 19033.810547
implicit_motion_alignment.3.blocks.0.mlp.2.bias: gradient norm = 1189.909180
implicit_motion_alignment.3.blocks.0.norm1.weight: gradient norm = 513.257996
implicit_motion_alignment.3.blocks.0.norm1.bias: gradient norm = 510.524811
implicit_motion_alignment.3.blocks.0.norm2.weight: gradient norm = 283.421783
implicit_motion_alignment.3.blocks.0.norm2.bias: gradient norm = 256.920807
implicit_motion_alignment.3.blocks.1.attention.in_proj_weight: gradient norm = 11401.266602
implicit_motion_alignment.3.blocks.1.attention.in_proj_bias: gradient norm = 503.946625
implicit_motion_alignment.3.blocks.1.attention.out_proj.weight: gradient norm = 14275.846680
implicit_motion_alignment.3.blocks.1.attention.out_proj.bias: gradient norm = 878.419495
implicit_motion_alignment.3.blocks.1.mlp.0.weight: gradient norm = 6948.125000
implicit_motion_alignment.3.blocks.1.mlp.0.bias: gradient norm = 306.458954
implicit_motion_alignment.3.blocks.1.mlp.2.weight: gradient norm = 13208.842773
implicit_motion_alignment.3.blocks.1.mlp.2.bias: gradient norm = 846.214172
implicit_motion_alignment.3.blocks.1.norm1.weight: gradient norm = 374.208740
implicit_motion_alignment.3.blocks.1.norm1.bias: gradient norm = 352.549347
implicit_motion_alignment.3.blocks.1.norm2.weight: gradient norm = 181.769089
implicit_motion_alignment.3.blocks.1.norm2.bias: gradient norm = 180.216583
implicit_motion_alignment.3.blocks.2.attention.in_proj_weight: gradient norm = 9266.676758
implicit_motion_alignment.3.blocks.2.attention.in_proj_bias: gradient norm = 409.569305
implicit_motion_alignment.3.blocks.2.attention.out_proj.weight: gradient norm = 11665.613281
implicit_motion_alignment.3.blocks.2.attention.out_proj.bias: gradient norm = 741.341492
implicit_motion_alignment.3.blocks.2.mlp.0.weight: gradient norm = 5668.476562
implicit_motion_alignment.3.blocks.2.mlp.0.bias: gradient norm = 249.987656
implicit_motion_alignment.3.blocks.2.mlp.2.weight: gradient norm = 11216.748047
implicit_motion_alignment.3.blocks.2.mlp.2.bias: gradient norm = 721.428223
implicit_motion_alignment.3.blocks.2.norm1.weight: gradient norm = 284.530792
implicit_motion_alignment.3.blocks.2.norm1.bias: gradient norm = 281.622528
implicit_motion_alignment.3.blocks.2.norm2.weight: gradient norm = 137.971054
implicit_motion_alignment.3.blocks.2.norm2.bias: gradient norm = 138.916519
implicit_motion_alignment.3.blocks.3.attention.in_proj_weight: gradient norm = 8320.942383
implicit_motion_alignment.3.blocks.3.attention.in_proj_bias: gradient norm = 367.763306
implicit_motion_alignment.3.blocks.3.attention.out_proj.weight: gradient norm = 9882.198242
implicit_motion_alignment.3.blocks.3.attention.out_proj.bias: gradient norm = 627.369751
implicit_motion_alignment.3.blocks.3.mlp.0.weight: gradient norm = 5058.784180
implicit_motion_alignment.3.blocks.3.mlp.0.bias: gradient norm = 223.123825
implicit_motion_alignment.3.blocks.3.mlp.2.weight: gradient norm = 9312.669922
implicit_motion_alignment.3.blocks.3.mlp.2.bias: gradient norm = 612.114624
implicit_motion_alignment.3.blocks.3.norm1.weight: gradient norm = 274.168304
implicit_motion_alignment.3.blocks.3.norm1.bias: gradient norm = 261.761627
implicit_motion_alignment.3.blocks.3.norm2.weight: gradient norm = 128.616287
implicit_motion_alignment.3.blocks.3.norm2.bias: gradient norm = 126.300209
frame_decoder.upconv_blocks.0.conv1.weight: gradient norm = 60592.308594
frame_decoder.upconv_blocks.0.conv1.bias: gradient norm = 0.001808
frame_decoder.upconv_blocks.0.bn1.weight: gradient norm = 363.514557
frame_decoder.upconv_blocks.0.bn1.bias: gradient norm = 327.577881
frame_decoder.upconv_blocks.0.conv2.weight: gradient norm = 45056.128906
frame_decoder.upconv_blocks.0.conv2.bias: gradient norm = 411.357971
frame_decoder.upconv_blocks.0.feat_res_block1.bn1.weight: gradient norm = 267.631134
frame_decoder.upconv_blocks.0.feat_res_block1.bn1.bias: gradient norm = 229.369659
frame_decoder.upconv_blocks.0.feat_res_block1.conv1.weight: gradient norm = 32624.078125
frame_decoder.upconv_blocks.0.feat_res_block1.conv1.bias: gradient norm = 0.000216
frame_decoder.upconv_blocks.0.feat_res_block1.bn2.weight: gradient norm = 234.035675
frame_decoder.upconv_blocks.0.feat_res_block1.bn2.bias: gradient norm = 237.045746
frame_decoder.upconv_blocks.0.feat_res_block1.conv2.weight: gradient norm = 28679.394531
frame_decoder.upconv_blocks.0.feat_res_block1.conv2.bias: gradient norm = 411.357941
frame_decoder.upconv_blocks.0.feat_res_block2.bn1.weight: gradient norm = 172.110153
frame_decoder.upconv_blocks.0.feat_res_block2.bn1.bias: gradient norm = 102.468239
frame_decoder.upconv_blocks.0.feat_res_block2.conv1.weight: gradient norm = 20867.496094
frame_decoder.upconv_blocks.0.feat_res_block2.conv1.bias: gradient norm = 0.000092
frame_decoder.upconv_blocks.0.feat_res_block2.bn2.weight: gradient norm = 156.510025
frame_decoder.upconv_blocks.0.feat_res_block2.bn2.bias: gradient norm = 152.596313
frame_decoder.upconv_blocks.0.feat_res_block2.conv2.weight: gradient norm = 18507.460938
frame_decoder.upconv_blocks.0.feat_res_block2.conv2.bias: gradient norm = 258.065430
frame_decoder.upconv_blocks.1.conv1.weight: gradient norm = 52354.042969
frame_decoder.upconv_blocks.1.conv1.bias: gradient norm = 0.000584
frame_decoder.upconv_blocks.1.bn1.weight: gradient norm = 270.254639
frame_decoder.upconv_blocks.1.bn1.bias: gradient norm = 249.904663
frame_decoder.upconv_blocks.1.conv2.weight: gradient norm = 30800.742188
frame_decoder.upconv_blocks.1.conv2.bias: gradient norm = 221.023911
frame_decoder.upconv_blocks.1.feat_res_block1.bn1.weight: gradient norm = 179.602097
frame_decoder.upconv_blocks.1.feat_res_block1.bn1.bias: gradient norm = 154.816086
frame_decoder.upconv_blocks.1.feat_res_block1.conv1.weight: gradient norm = 20695.675781
frame_decoder.upconv_blocks.1.feat_res_block1.conv1.bias: gradient norm = 0.000206
frame_decoder.upconv_blocks.1.feat_res_block1.bn2.weight: gradient norm = 155.480011
frame_decoder.upconv_blocks.1.feat_res_block1.bn2.bias: gradient norm = 139.650452
frame_decoder.upconv_blocks.1.feat_res_block1.conv2.weight: gradient norm = 18071.796875
frame_decoder.upconv_blocks.1.feat_res_block1.conv2.bias: gradient norm = 221.023926
frame_decoder.upconv_blocks.1.feat_res_block2.bn1.weight: gradient norm = 120.884003
frame_decoder.upconv_blocks.1.feat_res_block2.bn1.bias: gradient norm = 67.898956
frame_decoder.upconv_blocks.1.feat_res_block2.conv1.weight: gradient norm = 14481.229492
frame_decoder.upconv_blocks.1.feat_res_block2.conv1.bias: gradient norm = 0.000118
frame_decoder.upconv_blocks.1.feat_res_block2.bn2.weight: gradient norm = 106.740341
frame_decoder.upconv_blocks.1.feat_res_block2.bn2.bias: gradient norm = 96.545944
frame_decoder.upconv_blocks.1.feat_res_block2.conv2.weight: gradient norm = 12700.120117
frame_decoder.upconv_blocks.1.feat_res_block2.conv2.bias: gradient norm = 161.285233
frame_decoder.upconv_blocks.2.conv1.weight: gradient norm = 25584.107422
frame_decoder.upconv_blocks.2.conv1.bias: gradient norm = 0.000641
frame_decoder.upconv_blocks.2.bn1.weight: gradient norm = 144.211197
frame_decoder.upconv_blocks.2.bn1.bias: gradient norm = 124.190735
frame_decoder.upconv_blocks.2.conv2.weight: gradient norm = 12412.494141
frame_decoder.upconv_blocks.2.conv2.bias: gradient norm = 122.391388
frame_decoder.upconv_blocks.2.feat_res_block1.bn1.weight: gradient norm = 106.961502
frame_decoder.upconv_blocks.2.feat_res_block1.bn1.bias: gradient norm = 82.072807
frame_decoder.upconv_blocks.2.feat_res_block1.conv1.weight: gradient norm = 8421.174805
frame_decoder.upconv_blocks.2.feat_res_block1.conv1.bias: gradient norm = 0.000242
frame_decoder.upconv_blocks.2.feat_res_block1.bn2.weight: gradient norm = 94.342499
frame_decoder.upconv_blocks.2.feat_res_block1.bn2.bias: gradient norm = 82.140717
frame_decoder.upconv_blocks.2.feat_res_block1.conv2.weight: gradient norm = 7411.430176
frame_decoder.upconv_blocks.2.feat_res_block1.conv2.bias: gradient norm = 122.391388
frame_decoder.upconv_blocks.2.feat_res_block2.bn1.weight: gradient norm = 71.750580
frame_decoder.upconv_blocks.2.feat_res_block2.bn1.bias: gradient norm = 38.870594
frame_decoder.upconv_blocks.2.feat_res_block2.conv1.weight: gradient norm = 6024.892090
frame_decoder.upconv_blocks.2.feat_res_block2.conv1.bias: gradient norm = 0.000128
frame_decoder.upconv_blocks.2.feat_res_block2.bn2.weight: gradient norm = 65.086143
frame_decoder.upconv_blocks.2.feat_res_block2.bn2.bias: gradient norm = 55.891445
frame_decoder.upconv_blocks.2.feat_res_block2.conv2.weight: gradient norm = 5477.744141
frame_decoder.upconv_blocks.2.feat_res_block2.conv2.bias: gradient norm = 100.115128
frame_decoder.upconv_blocks.3.conv1.weight: gradient norm = 13759.450195
frame_decoder.upconv_blocks.3.conv1.bias: gradient norm = 0.000994
frame_decoder.upconv_blocks.3.bn1.weight: gradient norm = 143.252609
frame_decoder.upconv_blocks.3.bn1.bias: gradient norm = 107.856239
frame_decoder.upconv_blocks.3.conv2.weight: gradient norm = 7888.992188
frame_decoder.upconv_blocks.3.conv2.bias: gradient norm = 176.174347
frame_decoder.upconv_blocks.3.feat_res_block1.bn1.weight: gradient norm = 114.358330
frame_decoder.upconv_blocks.3.feat_res_block1.bn1.bias: gradient norm = 72.426872
frame_decoder.upconv_blocks.3.feat_res_block1.conv1.weight: gradient norm = 5829.777832
frame_decoder.upconv_blocks.3.feat_res_block1.conv1.bias: gradient norm = 0.000334
frame_decoder.upconv_blocks.3.feat_res_block1.bn2.weight: gradient norm = 114.239845
frame_decoder.upconv_blocks.3.feat_res_block1.bn2.bias: gradient norm = 74.872246
frame_decoder.upconv_blocks.3.feat_res_block1.conv2.weight: gradient norm = 6110.258789
frame_decoder.upconv_blocks.3.feat_res_block1.conv2.bias: gradient norm = 176.174438
frame_decoder.upconv_blocks.3.feat_res_block2.bn1.weight: gradient norm = 104.959328
frame_decoder.upconv_blocks.3.feat_res_block2.bn1.bias: gradient norm = 38.032383
frame_decoder.upconv_blocks.3.feat_res_block2.conv1.weight: gradient norm = 5385.477051
frame_decoder.upconv_blocks.3.feat_res_block2.conv1.bias: gradient norm = 0.000245
frame_decoder.upconv_blocks.3.feat_res_block2.bn2.weight: gradient norm = 122.739899
frame_decoder.upconv_blocks.3.feat_res_block2.bn2.bias: gradient norm = 61.308399
frame_decoder.upconv_blocks.3.feat_res_block2.conv2.weight: gradient norm = 7331.392090
frame_decoder.upconv_blocks.3.feat_res_block2.conv2.bias: gradient norm = 112.174393
frame_decoder.upconv_blocks.4.conv1.weight: gradient norm = 14409.164062
frame_decoder.upconv_blocks.4.conv1.bias: gradient norm = 0.004688
frame_decoder.upconv_blocks.4.bn1.weight: gradient norm = 1713.187866
frame_decoder.upconv_blocks.4.bn1.bias: gradient norm = 2384.170654
frame_decoder.upconv_blocks.4.conv2.weight: gradient norm = 71547.648438
frame_decoder.upconv_blocks.4.conv2.bias: gradient norm = 8161.481445
frame_decoder.upconv_blocks.4.feat_res_block1.bn1.weight: gradient norm = 196.721848
frame_decoder.upconv_blocks.4.feat_res_block1.bn1.bias: gradient norm = 129.611633
frame_decoder.upconv_blocks.4.feat_res_block1.conv1.weight: gradient norm = 8289.049805
frame_decoder.upconv_blocks.4.feat_res_block1.conv1.bias: gradient norm = 0.002942
frame_decoder.upconv_blocks.4.feat_res_block1.bn2.weight: gradient norm = 1934.109497
frame_decoder.upconv_blocks.4.feat_res_block1.bn2.bias: gradient norm = 2687.408691
frame_decoder.upconv_blocks.4.feat_res_block1.conv2.weight: gradient norm = 70619.046875
frame_decoder.upconv_blocks.4.feat_res_block1.conv2.bias: gradient norm = 8161.479980
frame_decoder.upconv_blocks.4.feat_res_block2.bn1.weight: gradient norm = 168.980469
frame_decoder.upconv_blocks.4.feat_res_block2.bn1.bias: gradient norm = 66.042244
frame_decoder.upconv_blocks.4.feat_res_block2.conv1.weight: gradient norm = 6858.655762
frame_decoder.upconv_blocks.4.feat_res_block2.conv1.bias: gradient norm = 0.001895
frame_decoder.upconv_blocks.4.feat_res_block2.bn2.weight: gradient norm = 2277.798340
frame_decoder.upconv_blocks.4.feat_res_block2.bn2.bias: gradient norm = 3192.185303
frame_decoder.upconv_blocks.4.feat_res_block2.conv2.weight: gradient norm = 92337.460938
frame_decoder.upconv_blocks.4.feat_res_block2.conv2.bias: gradient norm = 10983.710938
frame_decoder.feat_blocks.0.0.bn1.weight: gradient norm = 914.015198
frame_decoder.feat_blocks.0.0.bn1.bias: gradient norm = 1762.393555
frame_decoder.feat_blocks.0.0.conv1.weight: gradient norm = 112018.046875
frame_decoder.feat_blocks.0.0.conv1.bias: gradient norm = 0.001068
frame_decoder.feat_blocks.0.0.bn2.weight: gradient norm = 882.649109
frame_decoder.feat_blocks.0.0.bn2.bias: gradient norm = 709.563904
frame_decoder.feat_blocks.0.0.conv2.weight: gradient norm = 99750.523438
frame_decoder.feat_blocks.0.0.conv2.bias: gradient norm = 1457.059082
frame_decoder.feat_blocks.0.1.bn1.weight: gradient norm = 304.796997
frame_decoder.feat_blocks.0.1.bn1.bias: gradient norm = 164.815933
frame_decoder.feat_blocks.0.1.conv1.weight: gradient norm = 40088.449219
frame_decoder.feat_blocks.0.1.conv1.bias: gradient norm = 0.000141
frame_decoder.feat_blocks.0.1.bn2.weight: gradient norm = 287.639709
frame_decoder.feat_blocks.0.1.bn2.bias: gradient norm = 246.042969
frame_decoder.feat_blocks.0.1.conv2.weight: gradient norm = 34703.417969
frame_decoder.feat_blocks.0.1.conv2.bias: gradient norm = 348.631317
frame_decoder.feat_blocks.0.2.bn1.weight: gradient norm = 203.449966
frame_decoder.feat_blocks.0.2.bn1.bias: gradient norm = 134.795212
frame_decoder.feat_blocks.0.2.conv1.weight: gradient norm = 24610.287109
frame_decoder.feat_blocks.0.2.conv1.bias: gradient norm = 0.000090
frame_decoder.feat_blocks.0.2.bn2.weight: gradient norm = 186.012344
frame_decoder.feat_blocks.0.2.bn2.bias: gradient norm = 160.312744
frame_decoder.feat_blocks.0.2.conv2.weight: gradient norm = 21716.519531
frame_decoder.feat_blocks.0.2.conv2.bias: gradient norm = 234.856018
frame_decoder.feat_blocks.1.0.bn1.weight: gradient norm = 367.177979
frame_decoder.feat_blocks.1.0.bn1.bias: gradient norm = 916.964172
frame_decoder.feat_blocks.1.0.conv1.weight: gradient norm = 28024.142578
frame_decoder.feat_blocks.1.0.conv1.bias: gradient norm = 0.001249
frame_decoder.feat_blocks.1.0.bn2.weight: gradient norm = 296.687073
frame_decoder.feat_blocks.1.0.bn2.bias: gradient norm = 233.259674
frame_decoder.feat_blocks.1.0.conv2.weight: gradient norm = 24733.443359
frame_decoder.feat_blocks.1.0.conv2.bias: gradient norm = 436.446655
frame_decoder.feat_blocks.1.1.bn1.weight: gradient norm = 152.548508
frame_decoder.feat_blocks.1.1.bn1.bias: gradient norm = 72.520844
frame_decoder.feat_blocks.1.1.conv1.weight: gradient norm = 12899.326172
frame_decoder.feat_blocks.1.1.conv1.bias: gradient norm = 0.000132
frame_decoder.feat_blocks.1.1.bn2.weight: gradient norm = 133.931915
frame_decoder.feat_blocks.1.1.bn2.bias: gradient norm = 110.107811
frame_decoder.feat_blocks.1.1.conv2.weight: gradient norm = 11448.852539
frame_decoder.feat_blocks.1.1.conv2.bias: gradient norm = 161.139908
frame_decoder.feat_blocks.1.2.bn1.weight: gradient norm = 96.466309
frame_decoder.feat_blocks.1.2.bn1.bias: gradient norm = 62.471256
frame_decoder.feat_blocks.1.2.conv1.weight: gradient norm = 8062.954102
frame_decoder.feat_blocks.1.2.conv1.bias: gradient norm = 0.000090
frame_decoder.feat_blocks.1.2.bn2.weight: gradient norm = 85.917229
frame_decoder.feat_blocks.1.2.bn2.bias: gradient norm = 74.801933
frame_decoder.feat_blocks.1.2.conv2.weight: gradient norm = 7259.808594
frame_decoder.feat_blocks.1.2.conv2.bias: gradient norm = 98.378716
frame_decoder.feat_blocks.2.0.bn1.weight: gradient norm = 273.806976
frame_decoder.feat_blocks.2.0.bn1.bias: gradient norm = 114.268311
frame_decoder.feat_blocks.2.0.conv1.weight: gradient norm = 15457.373047
frame_decoder.feat_blocks.2.0.conv1.bias: gradient norm = 0.000337
frame_decoder.feat_blocks.2.0.bn2.weight: gradient norm = 210.567581
frame_decoder.feat_blocks.2.0.bn2.bias: gradient norm = 99.050667
frame_decoder.feat_blocks.2.0.conv2.weight: gradient norm = 13873.974609
frame_decoder.feat_blocks.2.0.conv2.bias: gradient norm = 182.663452
frame_decoder.feat_blocks.2.1.bn1.weight: gradient norm = 91.382500
frame_decoder.feat_blocks.2.1.bn1.bias: gradient norm = 36.706001
frame_decoder.feat_blocks.2.1.conv1.weight: gradient norm = 5163.604492
frame_decoder.feat_blocks.2.1.conv1.bias: gradient norm = 0.000127
frame_decoder.feat_blocks.2.1.bn2.weight: gradient norm = 85.532173
frame_decoder.feat_blocks.2.1.bn2.bias: gradient norm = 55.942551
frame_decoder.feat_blocks.2.1.conv2.weight: gradient norm = 4767.778809
frame_decoder.feat_blocks.2.1.conv2.bias: gradient norm = 87.282433
frame_decoder.feat_blocks.2.2.bn1.weight: gradient norm = 52.966972
frame_decoder.feat_blocks.2.2.bn1.bias: gradient norm = 31.354267
frame_decoder.feat_blocks.2.2.conv1.weight: gradient norm = 3682.902832
frame_decoder.feat_blocks.2.2.conv1.bias: gradient norm = 0.000082
frame_decoder.feat_blocks.2.2.bn2.weight: gradient norm = 58.731979
frame_decoder.feat_blocks.2.2.bn2.bias: gradient norm = 46.208664
frame_decoder.feat_blocks.2.2.conv2.weight: gradient norm = 3915.986816
frame_decoder.feat_blocks.2.2.conv2.bias: gradient norm = 60.335125
frame_decoder.final_conv.0.weight: gradient norm = 219061.906250
frame_decoder.final_conv.0.bias: gradient norm = 28033.355469
mapping_network.net.0.weight: gradient norm = 13.773472
mapping_network.net.0.bias: gradient norm = 1.867836
mapping_network.net.2.weight: gradient norm = 25.532295
mapping_network.net.2.bias: gradient norm = 5.514368
mapping_network.net.4.weight: gradient norm = 16.174578
mapping_network.net.4.bias: gradient norm = 9.728312
mapping_network.net.6.weight: gradient norm = 18.081602
mapping_network.net.6.bias: gradient norm = 25.428263
mapping_network.net.8.weight: gradient norm = 26.132113
mapping_network.net.8.bias: gradient norm = 58.106331
mapping_network.net.10.weight: gradient norm = 76.844368
mapping_network.net.10.bias: gradient norm = 128.570923
mapping_network.net.12.weight: gradient norm = 157.641357
mapping_network.net.12.bias: gradient norm = 335.587311
mapping_network.net.14.weight: gradient norm = 265.274292
mapping_network.net.14.bias: gradient norm = 709.298401
Gradient flow test passed for IMFModel

Testing DenseFeatureEncoder
âš¾ DenseFeatureEncoder input shape: torch.Size([1, 3, 256, 256])
    After initial conv: torch.Size([1, 64, 256, 256])
    After down_block 1: torch.Size([1, 64, 128, 128])
    After down_block 2: torch.Size([1, 128, 64, 64])
    After down_block 3: torch.Size([1, 256, 32, 32])
    After down_block 4: torch.Size([1, 512, 16, 16])
    After down_block 5: torch.Size([1, 512, 8, 8])
    DenseFeatureEncoder output shapes: [torch.Size([1, 128, 64, 64]), torch.Size([1, 256, 32, 32]), torch.Size([1, 512, 16, 16]), torch.Size([1, 512, 8, 8])]
Number of feature maps: 4
Feature map 0 shape: torch.Size([1, 128, 64, 64])
Feature map 1 shape: torch.Size([1, 256, 32, 32])
Feature map 2 shape: torch.Size([1, 512, 16, 16])
Feature map 3 shape: torch.Size([1, 512, 8, 8])
initial_conv.0.weight: gradient norm = 170580.140625
initial_conv.0.bias: gradient norm = 0.054110
initial_conv.1.weight: gradient norm = 6165.351074
initial_conv.1.bias: gradient norm = 6034.975586
down_blocks.0.conv1.weight: gradient norm = 285275.687500
down_blocks.0.conv1.bias: gradient norm = 0.091466
down_blocks.0.bn1.weight: gradient norm = 6036.648438
down_blocks.0.bn1.bias: gradient norm = 4927.727051
down_blocks.0.conv2.weight: gradient norm = 237660.609375
down_blocks.0.conv2.bias: gradient norm = 4562.112793
down_blocks.0.feat_res_block1.bn1.weight: gradient norm = 5337.809082
down_blocks.0.feat_res_block1.bn1.bias: gradient norm = 4314.460938
down_blocks.0.feat_res_block1.conv1.weight: gradient norm = 182041.171875
down_blocks.0.feat_res_block1.conv1.bias: gradient norm = 0.019406
down_blocks.0.feat_res_block1.bn2.weight: gradient norm = 3893.249756
down_blocks.0.feat_res_block1.bn2.bias: gradient norm = 3130.116455
down_blocks.0.feat_res_block1.conv2.weight: gradient norm = 159164.750000
down_blocks.0.feat_res_block1.conv2.bias: gradient norm = 4562.115723
down_blocks.0.feat_res_block2.bn1.weight: gradient norm = 3046.994629
down_blocks.0.feat_res_block2.bn1.bias: gradient norm = 1648.698608
down_blocks.0.feat_res_block2.conv1.weight: gradient norm = 109667.984375
down_blocks.0.feat_res_block2.conv1.bias: gradient norm = 0.010872
down_blocks.0.feat_res_block2.bn2.weight: gradient norm = 2728.734375
down_blocks.0.feat_res_block2.bn2.bias: gradient norm = 2385.563477
down_blocks.0.feat_res_block2.conv2.weight: gradient norm = 97057.367188
down_blocks.0.feat_res_block2.conv2.bias: gradient norm = 3406.217041
down_blocks.1.conv1.weight: gradient norm = 104285.085938
down_blocks.1.conv1.bias: gradient norm = 0.029957
down_blocks.1.bn1.weight: gradient norm = 5692.065918
down_blocks.1.bn1.bias: gradient norm = 6687.843750
down_blocks.1.conv2.weight: gradient norm = 297176.281250
down_blocks.1.conv2.bias: gradient norm = 20222.232422
down_blocks.1.feat_res_block1.bn1.weight: gradient norm = 1791.831177
down_blocks.1.feat_res_block1.bn1.bias: gradient norm = 1511.256714
down_blocks.1.feat_res_block1.conv1.weight: gradient norm = 97327.398438
down_blocks.1.feat_res_block1.conv1.bias: gradient norm = 0.004952
down_blocks.1.feat_res_block1.bn2.weight: gradient norm = 6779.489746
down_blocks.1.feat_res_block1.bn2.bias: gradient norm = 6748.512695
down_blocks.1.feat_res_block1.conv2.weight: gradient norm = 279915.812500
down_blocks.1.feat_res_block1.conv2.bias: gradient norm = 20222.228516
down_blocks.1.feat_res_block2.bn1.weight: gradient norm = 1110.107666
down_blocks.1.feat_res_block2.bn1.bias: gradient norm = 536.505493
down_blocks.1.feat_res_block2.conv1.weight: gradient norm = 56697.941406
down_blocks.1.feat_res_block2.conv1.bias: gradient norm = 0.002878
down_blocks.1.feat_res_block2.bn2.weight: gradient norm = 9267.432617
down_blocks.1.feat_res_block2.bn2.bias: gradient norm = 9886.988281
down_blocks.1.feat_res_block2.conv2.weight: gradient norm = 411766.656250
down_blocks.1.feat_res_block2.conv2.bias: gradient norm = 30928.339844
down_blocks.2.conv1.weight: gradient norm = 53153.492188
down_blocks.2.conv1.bias: gradient norm = 0.006086
down_blocks.2.bn1.weight: gradient norm = 2233.302979
down_blocks.2.bn1.bias: gradient norm = 2593.960449
down_blocks.2.conv2.weight: gradient norm = 157487.921875
down_blocks.2.conv2.bias: gradient norm = 7832.495117
down_blocks.2.feat_res_block1.bn1.weight: gradient norm = 591.916809
down_blocks.2.feat_res_block1.bn1.bias: gradient norm = 532.923096
down_blocks.2.feat_res_block1.conv1.weight: gradient norm = 49761.398438
down_blocks.2.feat_res_block1.conv1.bias: gradient norm = 0.000891
down_blocks.2.feat_res_block1.bn2.weight: gradient norm = 2610.686279
down_blocks.2.feat_res_block1.bn2.bias: gradient norm = 2712.192139
down_blocks.2.feat_res_block1.conv2.weight: gradient norm = 150819.390625
down_blocks.2.feat_res_block1.conv2.bias: gradient norm = 7832.495117
down_blocks.2.feat_res_block2.bn1.weight: gradient norm = 323.865509
down_blocks.2.feat_res_block2.bn1.bias: gradient norm = 193.137772
down_blocks.2.feat_res_block2.conv1.weight: gradient norm = 28084.486328
down_blocks.2.feat_res_block2.conv1.bias: gradient norm = 0.000526
down_blocks.2.feat_res_block2.bn2.weight: gradient norm = 3094.646729
down_blocks.2.feat_res_block2.bn2.bias: gradient norm = 3292.751465
down_blocks.2.feat_res_block2.conv2.weight: gradient norm = 203646.796875
down_blocks.2.feat_res_block2.conv2.bias: gradient norm = 10972.582031
down_blocks.3.conv1.weight: gradient norm = 26711.464844
down_blocks.3.conv1.bias: gradient norm = 0.000967
down_blocks.3.bn1.weight: gradient norm = 760.239929
down_blocks.3.bn1.bias: gradient norm = 874.010071
down_blocks.3.conv2.weight: gradient norm = 74343.328125
down_blocks.3.conv2.bias: gradient norm = 2671.331055
down_blocks.3.feat_res_block1.bn1.weight: gradient norm = 228.775681
down_blocks.3.feat_res_block1.bn1.bias: gradient norm = 200.657623
down_blocks.3.feat_res_block1.conv1.weight: gradient norm = 26002.552734
down_blocks.3.feat_res_block1.conv1.bias: gradient norm = 0.000190
down_blocks.3.feat_res_block1.bn2.weight: gradient norm = 844.289490
down_blocks.3.feat_res_block1.bn2.bias: gradient norm = 834.922668
down_blocks.3.feat_res_block1.conv2.weight: gradient norm = 70752.148438
down_blocks.3.feat_res_block1.conv2.bias: gradient norm = 2671.330811
down_blocks.3.feat_res_block2.bn1.weight: gradient norm = 135.755676
down_blocks.3.feat_res_block2.bn1.bias: gradient norm = 82.133858
down_blocks.3.feat_res_block2.conv1.weight: gradient norm = 15811.669922
down_blocks.3.feat_res_block2.conv1.bias: gradient norm = 0.000133
down_blocks.3.feat_res_block2.bn2.weight: gradient norm = 1088.709961
down_blocks.3.feat_res_block2.bn2.bias: gradient norm = 1144.484863
down_blocks.3.feat_res_block2.conv2.weight: gradient norm = 99523.093750
down_blocks.3.feat_res_block2.conv2.bias: gradient norm = 3912.964355
down_blocks.4.conv1.weight: gradient norm = 12301.476562
down_blocks.4.conv1.bias: gradient norm = 0.000165
down_blocks.4.bn1.weight: gradient norm = 174.082657
down_blocks.4.bn1.bias: gradient norm = 193.936859
down_blocks.4.conv2.weight: gradient norm = 18991.263672
down_blocks.4.conv2.bias: gradient norm = 662.906006
down_blocks.4.feat_res_block1.bn1.weight: gradient norm = 76.198135
down_blocks.4.feat_res_block1.bn1.bias: gradient norm = 63.357056
down_blocks.4.feat_res_block1.conv1.weight: gradient norm = 8722.328125
down_blocks.4.feat_res_block1.conv1.bias: gradient norm = 0.000040
down_blocks.4.feat_res_block1.bn2.weight: gradient norm = 226.620544
down_blocks.4.feat_res_block1.bn2.bias: gradient norm = 219.217957
down_blocks.4.feat_res_block1.conv2.weight: gradient norm = 17212.603516
down_blocks.4.feat_res_block1.conv2.bias: gradient norm = 662.906006
down_blocks.4.feat_res_block2.bn1.weight: gradient norm = 46.697407
down_blocks.4.feat_res_block2.bn1.bias: gradient norm = 29.666679
down_blocks.4.feat_res_block2.conv1.weight: gradient norm = 5199.561523
down_blocks.4.feat_res_block2.conv1.bias: gradient norm = 0.000033
down_blocks.4.feat_res_block2.bn2.weight: gradient norm = 258.914001
down_blocks.4.feat_res_block2.bn2.bias: gradient norm = 268.383759
down_blocks.4.feat_res_block2.conv2.weight: gradient norm = 23115.939453
down_blocks.4.feat_res_block2.conv2.bias: gradient norm = 970.836731
Gradient flow test passed for DenseFeatureEncoder
Initializing ResNetFeatureExtractor with output_channels: [128, 256, 512, 512]

Testing ResNetFeatureExtractor
ðŸ‘Ÿ ResNetFeatureExtractor input shape: torch.Size([1, 3, 256, 256])
After layer0: torch.Size([1, 64, 64, 64])
After layer1: torch.Size([1, 256, 64, 64])
After adjust1: torch.Size([1, 128, 64, 64])
After layer2: torch.Size([1, 512, 32, 32])
After adjust2: torch.Size([1, 256, 32, 32])
After layer3: torch.Size([1, 1024, 16, 16])
After adjust3: torch.Size([1, 512, 16, 16])
After layer4: torch.Size([1, 2048, 8, 8])
After adjust4: torch.Size([1, 512, 8, 8])
ResNetFeatureExtractor output: 4 features
  Feature 1 shape: torch.Size([1, 128, 64, 64])
  Feature 2 shape: torch.Size([1, 256, 32, 32])
  Feature 3 shape: torch.Size([1, 512, 16, 16])
  Feature 4 shape: torch.Size([1, 512, 8, 8])
Number of feature maps: 4
Feature map 0 shape: torch.Size([1, 128, 64, 64])
Feature map 1 shape: torch.Size([1, 256, 32, 32])
Feature map 2 shape: torch.Size([1, 512, 16, 16])
Feature map 3 shape: torch.Size([1, 512, 8, 8])
resnet.conv1.weight: gradient norm = 4800.786621
resnet.bn1.weight: gradient norm = 1046.911987
resnet.bn1.bias: gradient norm = 144.213150
resnet.layer1.0.conv1.weight: gradient norm = 1544.625000
resnet.layer1.0.bn1.weight: gradient norm = 451.380371
resnet.layer1.0.bn1.bias: gradient norm = 287.169098
resnet.layer1.0.conv2.weight: gradient norm = 1826.635620
resnet.layer1.0.bn2.weight: gradient norm = 788.603394
resnet.layer1.0.bn2.bias: gradient norm = 308.688843
resnet.layer1.0.conv3.weight: gradient norm = 2771.479492
resnet.layer1.0.bn3.weight: gradient norm = 5670.572266
resnet.layer1.0.bn3.bias: gradient norm = 14051.648438
resnet.layer1.0.downsample.0.weight: gradient norm = 2745.248535
resnet.layer1.0.downsample.1.weight: gradient norm = 7435.537109
resnet.layer1.0.downsample.1.bias: gradient norm = 14051.648438
resnet.layer1.1.conv1.weight: gradient norm = 686.393311
resnet.layer1.1.bn1.weight: gradient norm = 324.551392
resnet.layer1.1.bn1.bias: gradient norm = 198.787979
resnet.layer1.1.conv2.weight: gradient norm = 1350.724609
resnet.layer1.1.bn2.weight: gradient norm = 494.487000
resnet.layer1.1.bn2.bias: gradient norm = 359.002625
resnet.layer1.1.conv3.weight: gradient norm = 1761.334351
resnet.layer1.1.bn3.weight: gradient norm = 5458.712402
resnet.layer1.1.bn3.bias: gradient norm = 18945.775391
resnet.layer1.2.conv1.weight: gradient norm = 858.605835
resnet.layer1.2.bn1.weight: gradient norm = 332.684875
resnet.layer1.2.bn1.bias: gradient norm = 257.751526
resnet.layer1.2.conv2.weight: gradient norm = 1588.494629
resnet.layer1.2.bn2.weight: gradient norm = 538.706299
resnet.layer1.2.bn2.bias: gradient norm = 387.532867
resnet.layer1.2.conv3.weight: gradient norm = 1951.876831
resnet.layer1.2.bn3.weight: gradient norm = 4898.749512
resnet.layer1.2.bn3.bias: gradient norm = 21995.453125
resnet.layer2.0.conv1.weight: gradient norm = 2023.863647
resnet.layer2.0.bn1.weight: gradient norm = 510.163147
resnet.layer2.0.bn1.bias: gradient norm = 446.813141
resnet.layer2.0.conv2.weight: gradient norm = 3274.617188
resnet.layer2.0.bn2.weight: gradient norm = 625.217224
resnet.layer2.0.bn2.bias: gradient norm = 421.059875
resnet.layer2.0.conv3.weight: gradient norm = 2471.518799
resnet.layer2.0.bn3.weight: gradient norm = 2157.436523
resnet.layer2.0.bn3.bias: gradient norm = 3166.544922
resnet.layer2.0.downsample.0.weight: gradient norm = 2595.494385
resnet.layer2.0.downsample.1.weight: gradient norm = 2252.645508
resnet.layer2.0.downsample.1.bias: gradient norm = 3166.544922
resnet.layer2.1.conv1.weight: gradient norm = 1008.842529
resnet.layer2.1.bn1.weight: gradient norm = 361.865540
resnet.layer2.1.bn1.bias: gradient norm = 206.605804
resnet.layer2.1.conv2.weight: gradient norm = 1664.446411
resnet.layer2.1.bn2.weight: gradient norm = 388.908600
resnet.layer2.1.bn2.bias: gradient norm = 298.457458
resnet.layer2.1.conv3.weight: gradient norm = 1785.173706
resnet.layer2.1.bn3.weight: gradient norm = 1831.097534
resnet.layer2.1.bn3.bias: gradient norm = 3933.596191
resnet.layer2.2.conv1.weight: gradient norm = 2064.628174
resnet.layer2.2.bn1.weight: gradient norm = 493.342285
resnet.layer2.2.bn1.bias: gradient norm = 389.481293
resnet.layer2.2.conv2.weight: gradient norm = 2773.337646
resnet.layer2.2.bn2.weight: gradient norm = 518.756042
resnet.layer2.2.bn2.bias: gradient norm = 398.764740
resnet.layer2.2.conv3.weight: gradient norm = 2332.246826
resnet.layer2.2.bn3.weight: gradient norm = 2333.118652
resnet.layer2.2.bn3.bias: gradient norm = 4345.148438
resnet.layer2.3.conv1.weight: gradient norm = 2639.072021
resnet.layer2.3.bn1.weight: gradient norm = 460.071503
resnet.layer2.3.bn1.bias: gradient norm = 331.195435
resnet.layer2.3.conv2.weight: gradient norm = 3077.151123
resnet.layer2.3.bn2.weight: gradient norm = 585.600281
resnet.layer2.3.bn2.bias: gradient norm = 456.634857
resnet.layer2.3.conv3.weight: gradient norm = 2201.639404
resnet.layer2.3.bn3.weight: gradient norm = 2194.386963
resnet.layer2.3.bn3.bias: gradient norm = 5649.107422
resnet.layer3.0.conv1.weight: gradient norm = 3144.827881
resnet.layer3.0.bn1.weight: gradient norm = 695.798462
resnet.layer3.0.bn1.bias: gradient norm = 537.638306
resnet.layer3.0.conv2.weight: gradient norm = 4617.796387
resnet.layer3.0.bn2.weight: gradient norm = 536.139526
resnet.layer3.0.bn2.bias: gradient norm = 412.010681
resnet.layer3.0.conv3.weight: gradient norm = 2923.624756
resnet.layer3.0.bn3.weight: gradient norm = 848.097534
resnet.layer3.0.bn3.bias: gradient norm = 709.832764
resnet.layer3.0.downsample.0.weight: gradient norm = 3570.838135
resnet.layer3.0.downsample.1.weight: gradient norm = 797.042908
resnet.layer3.0.downsample.1.bias: gradient norm = 709.832764
resnet.layer3.1.conv1.weight: gradient norm = 2758.773926
resnet.layer3.1.bn1.weight: gradient norm = 504.663666
resnet.layer3.1.bn1.bias: gradient norm = 387.509064
resnet.layer3.1.conv2.weight: gradient norm = 3717.543213
resnet.layer3.1.bn2.weight: gradient norm = 584.846558
resnet.layer3.1.bn2.bias: gradient norm = 439.839874
resnet.layer3.1.conv3.weight: gradient norm = 2612.463623
resnet.layer3.1.bn3.weight: gradient norm = 800.792175
resnet.layer3.1.bn3.bias: gradient norm = 802.272583
resnet.layer3.2.conv1.weight: gradient norm = 2662.187500
resnet.layer3.2.bn1.weight: gradient norm = 593.774841
resnet.layer3.2.bn1.bias: gradient norm = 457.772217
resnet.layer3.2.conv2.weight: gradient norm = 3410.258301
resnet.layer3.2.bn2.weight: gradient norm = 625.713013
resnet.layer3.2.bn2.bias: gradient norm = 470.226471
resnet.layer3.2.conv3.weight: gradient norm = 2476.610107
resnet.layer3.2.bn3.weight: gradient norm = 787.189880
resnet.layer3.2.bn3.bias: gradient norm = 865.619385
resnet.layer3.3.conv1.weight: gradient norm = 2778.352539
resnet.layer3.3.bn1.weight: gradient norm = 555.105225
resnet.layer3.3.bn1.bias: gradient norm = 411.569366
resnet.layer3.3.conv2.weight: gradient norm = 3350.435303
resnet.layer3.3.bn2.weight: gradient norm = 473.241455
resnet.layer3.3.bn2.bias: gradient norm = 367.464722
resnet.layer3.3.conv3.weight: gradient norm = 2241.077393
resnet.layer3.3.bn3.weight: gradient norm = 763.453979
resnet.layer3.3.bn3.bias: gradient norm = 904.271545
resnet.layer3.4.conv1.weight: gradient norm = 2628.679443
resnet.layer3.4.bn1.weight: gradient norm = 563.440918
resnet.layer3.4.bn1.bias: gradient norm = 419.874725
resnet.layer3.4.conv2.weight: gradient norm = 3023.427734
resnet.layer3.4.bn2.weight: gradient norm = 491.295654
resnet.layer3.4.bn2.bias: gradient norm = 338.390381
resnet.layer3.4.conv3.weight: gradient norm = 2069.013916
resnet.layer3.4.bn3.weight: gradient norm = 728.373901
resnet.layer3.4.bn3.bias: gradient norm = 954.144836
resnet.layer3.5.conv1.weight: gradient norm = 2297.656738
resnet.layer3.5.bn1.weight: gradient norm = 476.655090
resnet.layer3.5.bn1.bias: gradient norm = 351.150116
resnet.layer3.5.conv2.weight: gradient norm = 2732.465088
resnet.layer3.5.bn2.weight: gradient norm = 375.037262
resnet.layer3.5.bn2.bias: gradient norm = 284.882965
resnet.layer3.5.conv3.weight: gradient norm = 1963.578247
resnet.layer3.5.bn3.weight: gradient norm = 821.036682
resnet.layer3.5.bn3.bias: gradient norm = 1305.015503
resnet.layer4.0.conv1.weight: gradient norm = 2335.634277
resnet.layer4.0.bn1.weight: gradient norm = 477.492004
resnet.layer4.0.bn1.bias: gradient norm = 315.976013
resnet.layer4.0.conv2.weight: gradient norm = 3563.329834
resnet.layer4.0.bn2.weight: gradient norm = 314.583069
resnet.layer4.0.bn2.bias: gradient norm = 236.488907
resnet.layer4.0.conv3.weight: gradient norm = 2241.919678
resnet.layer4.0.bn3.weight: gradient norm = 215.211945
resnet.layer4.0.bn3.bias: gradient norm = 213.119644
resnet.layer4.0.downsample.0.weight: gradient norm = 2420.404297
resnet.layer4.0.downsample.1.weight: gradient norm = 175.796448
resnet.layer4.0.downsample.1.bias: gradient norm = 213.119644
resnet.layer4.1.conv1.weight: gradient norm = 2086.645508
resnet.layer4.1.bn1.weight: gradient norm = 389.913391
resnet.layer4.1.bn1.bias: gradient norm = 279.497681
resnet.layer4.1.conv2.weight: gradient norm = 3087.079834
resnet.layer4.1.bn2.weight: gradient norm = 300.808746
resnet.layer4.1.bn2.bias: gradient norm = 219.829819
resnet.layer4.1.conv3.weight: gradient norm = 1802.238037
resnet.layer4.1.bn3.weight: gradient norm = 255.878143
resnet.layer4.1.bn3.bias: gradient norm = 325.353119
resnet.layer4.2.conv1.weight: gradient norm = 1601.803223
resnet.layer4.2.bn1.weight: gradient norm = 306.606506
resnet.layer4.2.bn1.bias: gradient norm = 216.133408
resnet.layer4.2.conv2.weight: gradient norm = 2545.529541
resnet.layer4.2.bn2.weight: gradient norm = 182.802536
resnet.layer4.2.bn2.bias: gradient norm = 152.100525
resnet.layer4.2.conv3.weight: gradient norm = 1953.472778
resnet.layer4.2.bn3.weight: gradient norm = 290.827362
resnet.layer4.2.bn3.bias: gradient norm = 486.070496
Warning: No gradient for resnet.fc.weight
Warning: No gradient for resnet.fc.bias
adjust1.weight: gradient norm = 160524.125000
adjust1.bias: gradient norm = 46340.949219
adjust2.weight: gradient norm = 46795.097656
adjust2.bias: gradient norm = 16384.000000
adjust3.weight: gradient norm = 7931.419434
adjust3.bias: gradient norm = 5792.618652
adjust4.weight: gradient norm = 30131.992188
adjust4.bias: gradient norm = 1448.154663
Gradient flow test passed for ResNetFeatureExtractor

Testing gradient flow for LatentTokenEncoder
LatentTokenEncoder input shape: torch.Size([1, 3, 256, 256])
After initial conv and activation: torch.Size([1, 64, 256, 256])
After res_block 1: torch.Size([1, 128, 128, 128])
After res_block 2: torch.Size([1, 256, 64, 64])
After res_block 3: torch.Size([1, 512, 32, 32])
After res_block 4: torch.Size([1, 512, 16, 16])
After res_block 5: torch.Size([1, 512, 8, 8])
After res_block 6: torch.Size([1, 512, 4, 4])
After equalconv: torch.Size([1, 512, 4, 4])
After global average pooling: torch.Size([1, 512])
After linear layer 1: torch.Size([1, 512])
After linear layer 2: torch.Size([1, 512])
After linear layer 3: torch.Size([1, 512])
After linear layer 4: torch.Size([1, 512])
Final output: torch.Size([1, 32])
conv1.weight: gradient norm = 77.054909
conv1.bias: gradient norm = 8.658038
res_blocks.0.conv1.conv.weight: gradient norm = 249.353256
res_blocks.0.conv1.conv.bias: gradient norm = 0.000042
res_blocks.0.conv1.bn.weight: gradient norm = 4.572390
res_blocks.0.conv1.bn.bias: gradient norm = 3.931448
res_blocks.0.conv2.conv.weight: gradient norm = 289.736969
res_blocks.0.conv2.conv.bias: gradient norm = 0.000025
res_blocks.0.conv2.bn.weight: gradient norm = 4.126070
res_blocks.0.conv2.bn.bias: gradient norm = 3.600731
res_blocks.0.skip_conv.conv.weight: gradient norm = 206.902969
res_blocks.0.skip_conv.conv.bias: gradient norm = 0.000041
res_blocks.0.skip_conv.bn.weight: gradient norm = 3.918743
res_blocks.0.skip_conv.bn.bias: gradient norm = 3.363138
res_blocks.1.conv1.conv.weight: gradient norm = 260.609314
res_blocks.1.conv1.conv.bias: gradient norm = 0.000007
res_blocks.1.conv1.bn.weight: gradient norm = 3.573316
res_blocks.1.conv1.bn.bias: gradient norm = 2.891786
res_blocks.1.conv2.conv.weight: gradient norm = 304.705994
res_blocks.1.conv2.conv.bias: gradient norm = 0.000008
res_blocks.1.conv2.bn.weight: gradient norm = 3.090661
res_blocks.1.conv2.bn.bias: gradient norm = 2.567820
res_blocks.1.skip_conv.conv.weight: gradient norm = 213.828308
res_blocks.1.skip_conv.conv.bias: gradient norm = 0.000006
res_blocks.1.skip_conv.bn.weight: gradient norm = 2.974518
res_blocks.1.skip_conv.bn.bias: gradient norm = 2.654161
res_blocks.2.conv1.conv.weight: gradient norm = 275.692627
res_blocks.2.conv1.conv.bias: gradient norm = 0.000003
res_blocks.2.conv1.bn.weight: gradient norm = 2.680028
res_blocks.2.conv1.bn.bias: gradient norm = 2.330478
res_blocks.2.conv2.conv.weight: gradient norm = 322.229462
res_blocks.2.conv2.conv.bias: gradient norm = 0.000003
res_blocks.2.conv2.bn.weight: gradient norm = 2.274737
res_blocks.2.conv2.bn.bias: gradient norm = 1.934244
res_blocks.2.skip_conv.conv.weight: gradient norm = 226.909836
res_blocks.2.skip_conv.conv.bias: gradient norm = 0.000002
res_blocks.2.skip_conv.bn.weight: gradient norm = 2.229406
res_blocks.2.skip_conv.bn.bias: gradient norm = 1.926265
res_blocks.3.conv1.conv.weight: gradient norm = 294.384094
res_blocks.3.conv1.conv.bias: gradient norm = 0.000001
res_blocks.3.conv1.bn.weight: gradient norm = 2.085438
res_blocks.3.conv1.bn.bias: gradient norm = 1.841472
res_blocks.3.conv2.conv.weight: gradient norm = 244.549484
res_blocks.3.conv2.conv.bias: gradient norm = 0.000001
res_blocks.3.conv2.bn.weight: gradient norm = 1.783224
res_blocks.3.conv2.bn.bias: gradient norm = 1.472559
res_blocks.3.skip_conv.conv.weight: gradient norm = 239.638962
res_blocks.3.skip_conv.conv.bias: gradient norm = 0.000001
res_blocks.3.skip_conv.bn.weight: gradient norm = 1.741726
res_blocks.3.skip_conv.bn.bias: gradient norm = 1.479385
res_blocks.4.conv1.conv.weight: gradient norm = 222.197418
res_blocks.4.conv1.conv.bias: gradient norm = 0.000001
res_blocks.4.conv1.bn.weight: gradient norm = 1.537410
res_blocks.4.conv1.bn.bias: gradient norm = 1.350556
res_blocks.4.conv2.conv.weight: gradient norm = 186.724823
res_blocks.4.conv2.conv.bias: gradient norm = 0.000001
res_blocks.4.conv2.bn.weight: gradient norm = 1.329779
res_blocks.4.conv2.bn.bias: gradient norm = 1.189980
res_blocks.4.skip_conv.conv.weight: gradient norm = 185.488098
res_blocks.4.skip_conv.conv.bias: gradient norm = 0.000000
res_blocks.4.skip_conv.bn.weight: gradient norm = 1.376067
res_blocks.4.skip_conv.bn.bias: gradient norm = 1.273393
res_blocks.5.conv1.conv.weight: gradient norm = 175.495209
res_blocks.5.conv1.conv.bias: gradient norm = 0.000000
res_blocks.5.conv1.bn.weight: gradient norm = 1.415485
res_blocks.5.conv1.bn.bias: gradient norm = 1.291371
res_blocks.5.conv2.conv.weight: gradient norm = 155.555298
res_blocks.5.conv2.conv.bias: gradient norm = 0.000001
res_blocks.5.conv2.bn.weight: gradient norm = 3.850402
res_blocks.5.conv2.bn.bias: gradient norm = 4.637940
res_blocks.5.skip_conv.conv.weight: gradient norm = 159.404724
res_blocks.5.skip_conv.conv.bias: gradient norm = 0.000001
res_blocks.5.skip_conv.bn.weight: gradient norm = 3.816106
res_blocks.5.skip_conv.bn.bias: gradient norm = 4.661281
equalconv.conv.bias: gradient norm = 9.044192
equalconv.conv.weight_orig: gradient norm = 7.597575
linear_layers.0.linear.bias: gradient norm = 6.444403
linear_layers.0.linear.weight_orig: gradient norm = 7.694224
linear_layers.1.linear.bias: gradient norm = 6.159342
linear_layers.1.linear.weight_orig: gradient norm = 7.564744
linear_layers.2.linear.bias: gradient norm = 5.789473
linear_layers.2.linear.weight_orig: gradient norm = 6.603564
linear_layers.3.linear.bias: gradient norm = 5.712759
linear_layers.3.linear.weight_orig: gradient norm = 6.747198
final_linear.linear.bias: gradient norm = 5.656854
final_linear.linear.weight_orig: gradient norm = 6.717711
Gradient flow test passed for LatentTokenEncoder

Testing gradient flow for LatentTokenDecoder
const: gradient norm = 779.447388
style_conv_layers.0.conv.weight: gradient norm = 539.260986
style_conv_layers.0.style.weight: gradient norm = 7917.743652
style_conv_layers.0.style.bias: gradient norm = 1298.246460
style_conv_layers.1.conv.weight: gradient norm = 784.501160
style_conv_layers.1.style.weight: gradient norm = 7587.056641
style_conv_layers.1.style.bias: gradient norm = 1244.025024
style_conv_layers.2.conv.weight: gradient norm = 749.168640
style_conv_layers.2.style.weight: gradient norm = 8494.576172
style_conv_layers.2.style.bias: gradient norm = 1392.828125
style_conv_layers.3.conv.weight: gradient norm = 773.820923
style_conv_layers.3.style.weight: gradient norm = 7538.504395
style_conv_layers.3.style.bias: gradient norm = 1236.064209
style_conv_layers.4.conv.weight: gradient norm = 670.328613
style_conv_layers.4.style.weight: gradient norm = 7605.530762
style_conv_layers.4.style.bias: gradient norm = 1247.054565
style_conv_layers.5.conv.weight: gradient norm = 586.553040
style_conv_layers.5.style.weight: gradient norm = 7297.344238
style_conv_layers.5.style.bias: gradient norm = 1196.521729
style_conv_layers.6.conv.weight: gradient norm = 484.850189
style_conv_layers.6.style.weight: gradient norm = 5575.540527
style_conv_layers.6.style.bias: gradient norm = 914.203064
style_conv_layers.7.conv.weight: gradient norm = 370.123871
style_conv_layers.7.style.weight: gradient norm = 4175.150391
style_conv_layers.7.style.bias: gradient norm = 684.585693
style_conv_layers.8.conv.weight: gradient norm = 404.413025
style_conv_layers.8.style.weight: gradient norm = 4310.747559
style_conv_layers.8.style.bias: gradient norm = 706.819519
style_conv_layers.9.conv.weight: gradient norm = 412.654205
style_conv_layers.9.style.weight: gradient norm = 4258.424805
style_conv_layers.9.style.bias: gradient norm = 698.239868
style_conv_layers.10.conv.weight: gradient norm = 245.772919
style_conv_layers.10.style.weight: gradient norm = 3062.014404
style_conv_layers.10.style.bias: gradient norm = 502.068481
style_conv_layers.11.conv.weight: gradient norm = 292.453766
style_conv_layers.11.style.weight: gradient norm = 3329.650391
style_conv_layers.11.style.bias: gradient norm = 545.951904
style_conv_layers.12.conv.weight: gradient norm = 232.569595
style_conv_layers.12.style.weight: gradient norm = 2870.214111
style_conv_layers.12.style.bias: gradient norm = 470.619781
Gradient flow test passed for LatentTokenDecoder

Testing ImplicitMotionAlignment modules
Testing ImplicitMotionAlignment for dim=128, spatial_size=64
cross_attention.to_q.weight: gradient norm = 830.171265
cross_attention.to_q.bias: gradient norm = 2252.421143
cross_attention.to_k.weight: gradient norm = 2901.171387
cross_attention.to_k.bias: gradient norm = 0.000049
cross_attention.to_v.weight: gradient norm = 75491.593750
cross_attention.to_v.bias: gradient norm = 379057.437500
cross_attention.to_out.weight: gradient norm = 420455.531250
cross_attention.to_out.bias: gradient norm = 699836.250000
blocks.0.attention.in_proj_weight: gradient norm = 657236.187500
blocks.0.attention.in_proj_bias: gradient norm = 58207.203125
blocks.0.attention.out_proj.weight: gradient norm = 940164.500000
blocks.0.attention.out_proj.bias: gradient norm = 99538.054688
blocks.0.mlp.0.weight: gradient norm = 377064.687500
blocks.0.mlp.0.bias: gradient norm = 33329.140625
blocks.0.mlp.2.weight: gradient norm = 718948.000000
blocks.0.mlp.2.bias: gradient norm = 89722.859375
blocks.0.norm1.weight: gradient norm = 40633.195312
blocks.0.norm1.bias: gradient norm = 40290.886719
blocks.0.norm2.weight: gradient norm = 18535.234375
blocks.0.norm2.bias: gradient norm = 19941.255859
blocks.1.attention.in_proj_weight: gradient norm = 455065.500000
blocks.1.attention.in_proj_bias: gradient norm = 40223.574219
blocks.1.attention.out_proj.weight: gradient norm = 515086.906250
blocks.1.attention.out_proj.bias: gradient norm = 71307.281250
blocks.1.mlp.0.weight: gradient norm = 262347.656250
blocks.1.mlp.0.bias: gradient norm = 23188.937500
blocks.1.mlp.2.weight: gradient norm = 489467.781250
blocks.1.mlp.2.bias: gradient norm = 65084.519531
blocks.1.norm1.weight: gradient norm = 28883.685547
blocks.1.norm1.bias: gradient norm = 29371.042969
blocks.1.norm2.weight: gradient norm = 14049.950195
blocks.1.norm2.bias: gradient norm = 15103.478516
blocks.2.attention.in_proj_weight: gradient norm = 363645.718750
blocks.2.attention.in_proj_bias: gradient norm = 32142.544922
blocks.2.attention.out_proj.weight: gradient norm = 424813.187500
blocks.2.attention.out_proj.bias: gradient norm = 55891.292969
blocks.2.mlp.0.weight: gradient norm = 221795.640625
blocks.2.mlp.0.bias: gradient norm = 19604.423828
blocks.2.mlp.2.weight: gradient norm = 441987.250000
blocks.2.mlp.2.bias: gradient norm = 54326.875000
blocks.2.norm1.weight: gradient norm = 20070.251953
blocks.2.norm1.bias: gradient norm = 21361.015625
blocks.2.norm2.weight: gradient norm = 10427.683594
blocks.2.norm2.bias: gradient norm = 11241.589844
blocks.3.attention.in_proj_weight: gradient norm = 293086.843750
blocks.3.attention.in_proj_bias: gradient norm = 25905.779297
blocks.3.attention.out_proj.weight: gradient norm = 374198.656250
blocks.3.attention.out_proj.bias: gradient norm = 47259.253906
blocks.3.mlp.0.weight: gradient norm = 190921.375000
blocks.3.mlp.0.bias: gradient norm = 16875.392578
blocks.3.mlp.2.weight: gradient norm = 367157.281250
blocks.3.mlp.2.bias: gradient norm = 46340.949219
blocks.3.norm1.weight: gradient norm = 22470.152344
blocks.3.norm1.bias: gradient norm = 21038.578125
blocks.3.norm2.weight: gradient norm = 8983.610352
blocks.3.norm2.bias: gradient norm = 8902.168945
Gradient flow test passed for ImplicitMotionAlignment with dim=128
Testing ImplicitMotionAlignment for dim=256, spatial_size=32
cross_attention.to_q.weight: gradient norm = 1242.162476
cross_attention.to_q.bias: gradient norm = 1921.085205
cross_attention.to_k.weight: gradient norm = 3077.728271
cross_attention.to_k.bias: gradient norm = 0.000031
cross_attention.to_v.weight: gradient norm = 90357.109375
cross_attention.to_v.bias: gradient norm = 187364.156250
cross_attention.to_out.weight: gradient norm = 212677.296875
cross_attention.to_out.bias: gradient norm = 340579.593750
blocks.0.attention.in_proj_weight: gradient norm = 330534.437500
blocks.0.attention.in_proj_bias: gradient norm = 20790.921875
blocks.0.attention.out_proj.weight: gradient norm = 420269.468750
blocks.0.attention.out_proj.bias: gradient norm = 36665.308594
blocks.0.mlp.0.weight: gradient norm = 189268.734375
blocks.0.mlp.0.bias: gradient norm = 11830.186523
blocks.0.mlp.2.weight: gradient norm = 358124.156250
blocks.0.mlp.2.bias: gradient norm = 32148.673828
blocks.0.norm1.weight: gradient norm = 14946.377930
blocks.0.norm1.bias: gradient norm = 14309.821289
blocks.0.norm2.weight: gradient norm = 6713.513184
blocks.0.norm2.bias: gradient norm = 6847.319824
blocks.1.attention.in_proj_weight: gradient norm = 220829.062500
blocks.1.attention.in_proj_bias: gradient norm = 13802.628906
blocks.1.attention.out_proj.weight: gradient norm = 272784.562500
blocks.1.attention.out_proj.bias: gradient norm = 23890.263672
blocks.1.mlp.0.weight: gradient norm = 130541.664062
blocks.1.mlp.0.bias: gradient norm = 8159.210449
blocks.1.mlp.2.weight: gradient norm = 265748.250000
blocks.1.mlp.2.bias: gradient norm = 23132.369141
blocks.1.norm1.weight: gradient norm = 10471.356445
blocks.1.norm1.bias: gradient norm = 10546.423828
blocks.1.norm2.weight: gradient norm = 4646.239746
blocks.1.norm2.bias: gradient norm = 4485.238281
blocks.2.attention.in_proj_weight: gradient norm = 181167.000000
blocks.2.attention.in_proj_bias: gradient norm = 11323.332031
blocks.2.attention.out_proj.weight: gradient norm = 224253.875000
blocks.2.attention.out_proj.bias: gradient norm = 19564.628906
blocks.2.mlp.0.weight: gradient norm = 110451.617188
blocks.2.mlp.0.bias: gradient norm = 6903.474609
blocks.2.mlp.2.weight: gradient norm = 211880.218750
blocks.2.mlp.2.bias: gradient norm = 18862.037109
blocks.2.norm1.weight: gradient norm = 7891.067383
blocks.2.norm1.bias: gradient norm = 7728.061523
blocks.2.norm2.weight: gradient norm = 4577.380859
blocks.2.norm2.bias: gradient norm = 4303.497070
blocks.3.attention.in_proj_weight: gradient norm = 150842.578125
blocks.3.attention.in_proj_bias: gradient norm = 9427.898438
blocks.3.attention.out_proj.weight: gradient norm = 185588.765625
blocks.3.attention.out_proj.bias: gradient norm = 16886.332031
blocks.3.mlp.0.weight: gradient norm = 91945.140625
blocks.3.mlp.0.bias: gradient norm = 5746.728027
blocks.3.mlp.2.weight: gradient norm = 195401.046875
blocks.3.mlp.2.bias: gradient norm = 16384.000000
blocks.3.norm1.weight: gradient norm = 6441.775391
blocks.3.norm1.bias: gradient norm = 6424.875000
blocks.3.norm2.weight: gradient norm = 3628.647949
blocks.3.norm2.bias: gradient norm = 3575.992432
Gradient flow test passed for ImplicitMotionAlignment with dim=256
Testing ImplicitMotionAlignment for dim=512, spatial_size=16
cross_attention.to_q.weight: gradient norm = 2377.416504
cross_attention.to_q.bias: gradient norm = 1611.652100
cross_attention.to_k.weight: gradient norm = 3261.309814
cross_attention.to_k.bias: gradient norm = 0.000029
cross_attention.to_v.weight: gradient norm = 116167.125000
cross_attention.to_v.bias: gradient norm = 80549.664062
cross_attention.to_out.weight: gradient norm = 151539.265625
cross_attention.to_out.bias: gradient norm = 144888.562500
blocks.0.attention.in_proj_weight: gradient norm = 167920.296875
blocks.0.attention.in_proj_bias: gradient norm = 7592.502930
blocks.0.attention.out_proj.weight: gradient norm = 206169.390625
blocks.0.attention.out_proj.bias: gradient norm = 13067.125977
blocks.0.mlp.0.weight: gradient norm = 97404.921875
blocks.0.mlp.0.bias: gradient norm = 4305.809082
blocks.0.mlp.2.weight: gradient norm = 177578.906250
blocks.0.mlp.2.bias: gradient norm = 11957.936523
blocks.0.norm1.weight: gradient norm = 5181.068848
blocks.0.norm1.bias: gradient norm = 5329.598145
blocks.0.norm2.weight: gradient norm = 2524.885010
blocks.0.norm2.bias: gradient norm = 2552.831055
blocks.1.attention.in_proj_weight: gradient norm = 116449.585938
blocks.1.attention.in_proj_bias: gradient norm = 5147.482910
blocks.1.attention.out_proj.weight: gradient norm = 144568.343750
blocks.1.attention.out_proj.bias: gradient norm = 8931.921875
blocks.1.mlp.0.weight: gradient norm = 65922.906250
blocks.1.mlp.0.bias: gradient norm = 2913.866455
blocks.1.mlp.2.weight: gradient norm = 135285.375000
blocks.1.mlp.2.bias: gradient norm = 8303.961914
blocks.1.norm1.weight: gradient norm = 3756.583984
blocks.1.norm1.bias: gradient norm = 3501.456055
blocks.1.norm2.weight: gradient norm = 1656.396484
blocks.1.norm2.bias: gradient norm = 1739.926636
blocks.2.attention.in_proj_weight: gradient norm = 92338.960938
blocks.2.attention.in_proj_bias: gradient norm = 4081.290039
blocks.2.attention.out_proj.weight: gradient norm = 114180.890625
blocks.2.attention.out_proj.bias: gradient norm = 7064.245117
blocks.2.mlp.0.weight: gradient norm = 54397.640625
blocks.2.mlp.0.bias: gradient norm = 2404.385986
blocks.2.mlp.2.weight: gradient norm = 107140.007812
blocks.2.mlp.2.bias: gradient norm = 6784.312500
blocks.2.norm1.weight: gradient norm = 2845.508057
blocks.2.norm1.bias: gradient norm = 2693.060791
blocks.2.norm2.weight: gradient norm = 1350.523926
blocks.2.norm2.bias: gradient norm = 1371.093750
blocks.3.attention.in_proj_weight: gradient norm = 75547.195312
blocks.3.attention.in_proj_bias: gradient norm = 3339.017334
blocks.3.attention.out_proj.weight: gradient norm = 96877.164062
blocks.3.attention.out_proj.bias: gradient norm = 5954.127441
blocks.3.mlp.0.weight: gradient norm = 47230.671875
blocks.3.mlp.0.bias: gradient norm = 2087.548096
blocks.3.mlp.2.weight: gradient norm = 87877.218750
blocks.3.mlp.2.bias: gradient norm = 5792.618652
blocks.3.norm1.weight: gradient norm = 2362.469482
blocks.3.norm1.bias: gradient norm = 2427.172607
blocks.3.norm2.weight: gradient norm = 1180.518188
blocks.3.norm2.bias: gradient norm = 1207.247070
Gradient flow test passed for ImplicitMotionAlignment with dim=512
Testing ImplicitMotionAlignment for dim=512, spatial_size=8
cross_attention.to_q.weight: gradient norm = 1483.171021
cross_attention.to_q.bias: gradient norm = 503.627716
cross_attention.to_k.weight: gradient norm = 1681.210327
cross_attention.to_k.bias: gradient norm = 0.000013
cross_attention.to_v.weight: gradient norm = 36307.039062
cross_attention.to_v.bias: gradient norm = 13140.595703
cross_attention.to_out.weight: gradient norm = 36972.554688
cross_attention.to_out.bias: gradient norm = 22684.265625
blocks.0.attention.in_proj_weight: gradient norm = 36526.507812
blocks.0.attention.in_proj_bias: gradient norm = 1675.603149
blocks.0.attention.out_proj.weight: gradient norm = 48570.421875
blocks.0.attention.out_proj.bias: gradient norm = 2935.032471
blocks.0.mlp.0.weight: gradient norm = 21888.810547
blocks.0.mlp.0.bias: gradient norm = 967.929993
blocks.0.mlp.2.weight: gradient norm = 42141.007812
blocks.0.mlp.2.bias: gradient norm = 2692.202881
blocks.0.norm1.weight: gradient norm = 1168.022461
blocks.0.norm1.bias: gradient norm = 1184.776978
blocks.0.norm2.weight: gradient norm = 545.710815
blocks.0.norm2.bias: gradient norm = 545.764465
blocks.1.attention.in_proj_weight: gradient norm = 26525.279297
blocks.1.attention.in_proj_bias: gradient norm = 1172.923828
blocks.1.attention.out_proj.weight: gradient norm = 35669.054688
blocks.1.attention.out_proj.bias: gradient norm = 2056.190430
blocks.1.mlp.0.weight: gradient norm = 15664.407227
blocks.1.mlp.0.bias: gradient norm = 692.517212
blocks.1.mlp.2.weight: gradient norm = 30074.457031
blocks.1.mlp.2.bias: gradient norm = 1949.438843
blocks.1.norm1.weight: gradient norm = 761.614319
blocks.1.norm1.bias: gradient norm = 794.153076
blocks.1.norm2.weight: gradient norm = 374.636383
blocks.1.norm2.bias: gradient norm = 387.756561
blocks.2.attention.in_proj_weight: gradient norm = 21088.666016
blocks.2.attention.in_proj_bias: gradient norm = 932.268433
blocks.2.attention.out_proj.weight: gradient norm = 26868.851562
blocks.2.attention.out_proj.bias: gradient norm = 1685.688232
blocks.2.mlp.0.weight: gradient norm = 12992.422852
blocks.2.mlp.0.bias: gradient norm = 574.340698
blocks.2.mlp.2.weight: gradient norm = 26733.482422
blocks.2.mlp.2.bias: gradient norm = 1651.312622
blocks.2.norm1.weight: gradient norm = 637.058960
blocks.2.norm1.bias: gradient norm = 656.740479
blocks.2.norm2.weight: gradient norm = 316.993774
blocks.2.norm2.bias: gradient norm = 329.310150
blocks.3.attention.in_proj_weight: gradient norm = 19976.527344
blocks.3.attention.in_proj_bias: gradient norm = 883.036926
blocks.3.attention.out_proj.weight: gradient norm = 23130.628906
blocks.3.attention.out_proj.bias: gradient norm = 1480.547974
blocks.3.mlp.0.weight: gradient norm = 11456.120117
blocks.3.mlp.0.bias: gradient norm = 506.405090
blocks.3.mlp.2.weight: gradient norm = 23175.083984
blocks.3.mlp.2.bias: gradient norm = 1448.154663
blocks.3.norm1.weight: gradient norm = 642.471375
blocks.3.norm1.bias: gradient norm = 617.867859
blocks.3.norm2.weight: gradient norm = 265.104340
blocks.3.norm2.bias: gradient norm = 281.729309
Gradient flow test passed for ImplicitMotionAlignment with dim=512
