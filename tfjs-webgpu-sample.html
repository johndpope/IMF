<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TensorFlow.js WebGPU/WebGL IMF Encoder Demo</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.18.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgpu@3.18.0/dist/tf-backend-webgpu.min.js"></script>
    <style>
        .image-preview {
            max-width: 256px;
            max-height: 256px;
            margin: 10px 0;
        }
        #modelInfo, #errorLog {
            white-space: pre-wrap;
            font-family: monospace;
            background-color: #f0f0f0;
            padding: 10px;
            margin-top: 10px;
        }
    </style>
</head>
<body>
    <h1>TensorFlow.js WebGPU/WebGL IMF Encoder Demo</h1>
    <div>
        <label for="currentImage">Current Image:</label>
        <input type="file" id="currentImage" accept="image/*">
        <img id="currentPreview" class="image-preview">
    </div>
    <div>
        <label for="referenceImage">Reference Image:</label>
        <input type="file" id="referenceImage" accept="image/*">
        <img id="referencePreview" class="image-preview">
    </div>
    <button id="runInference">Run Inference</button>
    <div id="status">Initializing...</div>
    <div id="backendInfo"></div>
    <div id="modelInfo"></div>
    <div id="errorLog"></div>
    <div id="output"></div>

    <script>
        let model;

        async function loadModel() {
            const statusElement = document.getElementById('status');
            const backendInfoElement = document.getElementById('backendInfo');
            const modelInfoElement = document.getElementById('modelInfo');
            const errorLogElement = document.getElementById('errorLog');

            try {
                // Check available backends
                const availableBackends = tf.engine().registryFactory;
                backendInfoElement.innerHTML = '<h3>Available Backends:</h3>' + 
                    Object.keys(availableBackends).join(', ');

                // Try to set WebGPU backend, fall back to WebGL if not available
                if (tf.findBackend('webgpu')) {
                    await tf.setBackend('webgpu');
                } else if (tf.findBackend('webgl')) {
                    console.warn('WebGPU not available, falling back to WebGL');
                    await tf.setBackend('webgl');
                } else {
                    throw new Error('Neither WebGPU nor WebGL backend is available');
                }

                await tf.ready();
                console.log('Backend initialized:', tf.getBackend());
                backendInfoElement.innerHTML += '<p>Active Backend: ' + tf.getBackend() + '</p>';

                statusElement.textContent = 'Loading model...';
                
                // Attempt to fetch the model.json file
                const modelResponse = await tf.loadLayersModel('tfjs_imf_encoder/model.json');
                if (!modelResponse.ok) {
                    throw new Error(`HTTP error! status: ${modelResponse.status}`);
                }
                const modelJSON = await modelResponse.json();
                modelInfoElement.textContent = JSON.stringify(modelJSON, null, 2);

                model = await tf.loadGraphModel('tfjs_imf_encoder/model.json');
                statusElement.textContent = 'Model loaded successfully! You can now run inference.';
                
                console.log('Model structure:', model);
                modelInfoElement.textContent += '\n\nModel structure:\n' + JSON.stringify(model, null, 2);
            } catch (error) {
                statusElement.textContent = 'Error: ' + error.message;
                console.error('Error:', error);
                errorLogElement.textContent = 'Error stack:\n' + error.stack;
            }
        }

        async function processImage(file) {
            return new Promise((resolve, reject) => {
                const reader = new FileReader();
                reader.onload = async (e) => {
                    const img = new Image();
                    img.onload = async () => {
                        const tensor = tf.tidy(() => {
                            const tempTensor = tf.browser.fromPixels(img);
                            const resized = tf.image.resizeBilinear(tempTensor, [256, 256]);
                            const normalized = resized.div(255.0).sub(0.5).mul(2);
                            return normalized.expandDims(0);
                        });
                        resolve(tensor);
                    };
                    img.onerror = reject;
                    img.src = e.target.result;
                };
                reader.onerror = reject;
                reader.readAsDataURL(file);
            });
        }

        async function runInference() {
            const statusElement = document.getElementById('status');
            const outputElement = document.getElementById('output');

            try {
                statusElement.textContent = 'Processing images...';
                const currentImageFile = document.getElementById('currentImage').files[0];
                const referenceImageFile = document.getElementById('referenceImage').files[0];

                if (!currentImageFile || !referenceImageFile) {
                    throw new Error('Please select both current and reference images.');
                }

                const x_current = await processImage(currentImageFile);
                const x_reference = await processImage(referenceImageFile);

                statusElement.textContent = 'Running inference...';
                const result = await model.executeAsync({
                    'x_current': x_current,
                    'x_reference': x_reference
                });

                statusElement.textContent = 'Inference complete!';
                outputElement.innerHTML = '<h2>Output Shapes:</h2>';
                if (Array.isArray(result)) {
                    result.forEach((tensor, index) => {
                        outputElement.innerHTML += `<p>Output ${index + 1}: ${tensor.shape.join(' x ')}</p>`;
                    });
                } else {
                    outputElement.innerHTML += `<p>Output: ${result.shape.join(' x ')}</p>`;
                }

                // Clean up
                tf.dispose([x_current, x_reference, result]);

            } catch (error) {
                statusElement.textContent = 'Error: ' + error.message;
                console.error('Error:', error);
            }
        }

        function previewImage(input, previewId) {
            const preview = document.getElementById(previewId);
            const file = input.files[0];
            const reader = new FileReader();

            reader.onload = function (e) {
                preview.src = e.target.result;
            };

            if (file) {
                reader.readAsDataURL(file);
            }
        }

        window.onload = () => {
            loadModel();
            document.getElementById('currentImage').addEventListener('change', function() {
                previewImage(this, 'currentPreview');
            });
            document.getElementById('referenceImage').addEventListener('change', function() {
                previewImage(this, 'referencePreview');
            });
            document.getElementById('runInference').addEventListener('click', runInference);
        };
    </script>
</body>
</html>